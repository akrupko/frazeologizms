#!/usr/bin/env python3
"""
Script to analyze phraseological units and correct categorization based on semantic meaning.
This script focuses on the actual meaning of the complete expression, not individual words.
"""

import json
import re
from collections import defaultdict

def load_phrases():
    """Load phrases from the JSON file."""
    with open('table_phrases.json', 'r', encoding='utf-8') as f:
        data = json.load(f)
    return data

def analyze_semantic_categorization(data):
    """Analyze and suggest corrections based on semantic meaning."""
    phrases = data['phrases']
    categories = data['categories']
    
    corrections = []
    
    print("ðŸ” Analyzing semantic categorization...")
    print(f"Total phrases: {len(phrases)}")
    
    # Define semantic categorization rules based on meaning, not keywords
    semantic_rules = {
        'emotions_feelings': {
            'patterns': [
                r'Ñ€Ð°Ð´Ð¾ÑÑ‚|ÑÑ‡Ð°ÑÑ‚Ð»Ð¸Ð²|Ð²ÐµÑÐµÐ»|ÑÐ¼ÐµÑ…|ÑƒÐ»Ñ‹Ð±Ðº',
                r'Ð³Ñ€ÑƒÑÑ‚ÑŒ|Ð¿ÐµÑ‡Ð°Ð»ÑŒ|Ð³Ð¾Ñ€Ðµ|ÑÐ»ÐµÐ·|Ð¿Ð»Ð°Ñ‡|Ñ‚Ð¾ÑÐºÐ°',
                r'ÑÑ‚Ñ€Ð°Ñ…|Ð±Ð¾ÑÐ·Ð½|Ð¸ÑÐ¿ÑƒÐ³|ÑƒÐ¶Ð°Ñ|Ñ‚Ñ€ÑƒÑ',
                r'Ð³Ð½ÐµÐ²|Ð·Ð»Ð¾ÑÑ‚|ÑÑ€Ð¾ÑÑ‚|ÑÐµÑ€Ð´Ð¸Ñ‚',
                r'Ð»ÑŽÐ±Ð¾Ð²ÑŒ|Ð²Ð»ÑŽÐ±Ð»|ÑÑ‚Ñ€Ð°ÑÑ‚',
                r'Ð½ÐµÐ½Ð°Ð²Ð¸ÑÑ‚|Ð²Ñ€Ð°Ð¶Ð´ÐµÐ±',
                r'Ð²Ð¾Ð»Ð½ÐµÐ½Ð¸Ðµ|Ð±ÐµÑÐ¿Ð¾ÐºÐ¾Ð¹|Ñ‚Ñ€ÐµÐ²Ð¾Ð³',
                r'ÑÑ‚Ñ‹Ð´|ÑÑ€Ð°Ð¼|Ð¿Ð¾Ð·Ð¾Ñ€',
                r'Ð·Ð°Ð²Ð¸ÑÑ‚ÑŒ|Ñ€ÐµÐ²Ð½Ð¾ÑÑ‚',
                r'ÑƒÐ´Ð¸Ð²Ð»ÐµÐ½Ð¸Ðµ|Ð¸Ð·ÑƒÐ¼Ð»ÐµÐ½Ð¸Ðµ',
                r'ÑÐ¼Ð¾Ñ†Ð¸|Ñ‡ÑƒÐ²ÑÑ‚Ð²|Ð½Ð°ÑÑ‚Ñ€Ð¾ÐµÐ½Ð¸Ðµ'
            ],
            'meaning_indicators': [
                'ÑÐ¼Ð¾Ñ†Ð¸Ð¾Ð½Ð°Ð»ÑŒÐ½Ð¾Ðµ ÑÐ¾ÑÑ‚Ð¾ÑÐ½Ð¸Ðµ',
                'Ñ‡ÑƒÐ²ÑÑ‚Ð²Ð¾',
                'Ð¿ÐµÑ€ÐµÐ¶Ð¸Ð²Ð°Ð½Ð¸Ðµ',
                'Ð½Ð°ÑÑ‚Ñ€Ð¾ÐµÐ½Ð¸Ðµ',
                'Ð´ÑƒÑˆÐµÐ²Ð½Ð¾Ðµ Ð²Ð¾Ð»Ð½ÐµÐ½Ð¸Ðµ'
            ]
        },
        'body_parts': {
            'patterns': [
                r'Ð³Ð¾Ð»Ð¾Ð²|Ð±Ð°ÑˆÐº|Ñ‡ÐµÑ€ÐµÐ¿',
                r'Ð³Ð»Ð°Ð·|Ð¾ÐºÐ¾|Ð²Ð·Ð³Ð»ÑÐ´|Ð²Ð·Ð¾Ñ€',
                r'Ñ€ÑƒÐº|Ð»Ð°Ð´Ð¾Ð½|Ð¿Ð°Ð»ÑŒÑ†|ÐºÑƒÐ»Ð°Ðº',
                r'Ð½Ð¾Ð³|ÑÑ‚Ð¾Ð¿|Ð¿ÑÑ‚',
                r'ÑÐµÑ€Ð´Ñ†|Ð´ÑƒÑˆ',
                r'ÑÐ·Ñ‹Ðº|Ñ€Ð¾Ñ‚|Ð³ÑƒÐ±|Ð·ÑƒÐ±',
                r'ÑƒÑ…Ð¾|ÑÐ»ÑƒÑ…',
                r'Ð½Ð¾Ñ|Ð¾Ð±Ð¾Ð½ÑÐ½Ð¸Ðµ',
                r'Ð»Ð¸Ñ†Ð¾|Ð»Ð¸Ðº|Ñ‰ÐµÐº',
                r'ÑÐ¿Ð¸Ð½|Ð¿Ð»ÐµÑ‡|Ð³Ñ€ÑƒÐ´ÑŒ',
                r'Ð¶Ð¸Ð²Ð¾Ñ‚|Ð¶ÐµÐ»ÑƒÐ´Ð¾Ðº',
                r'ÑˆÐµÑ|Ð³Ð¾Ñ€Ð»Ð¾',
                r'Ð²Ð¾Ð»Ð¾Ñ|Ð±Ð¾Ñ€Ð¾Ð´Ð°',
                r'ÐºÐ¾Ð¶Ð°|Ñ‚ÐµÐ»Ð¾'
            ]
        },
        'animals': {
            'patterns': [
                r'ÐºÐ¾Ñ‚|ÐºÐ¾ÑˆÐº|ÐºÐ¾Ñ‚ÐµÐ½',
                r'ÑÐ¾Ð±Ð°Ðº|Ð¿ÐµÑ|Ñ‰ÐµÐ½Ð¾Ðº',
                r'Ð»Ð¾ÑˆÐ°Ð´|ÐºÐ¾Ð½ÑŒ|ÐºÐ¾Ð±Ñ‹Ð»|Ð¶ÐµÑ€ÐµÐ±ÐµÑ†',
                r'ÐºÐ¾Ñ€Ð¾Ð²Ð°|Ð±Ñ‹Ðº|Ñ‚ÐµÐ»ÐµÐ½',
                r'Ð²Ð¾Ð»Ðº|Ð²Ð¾Ð»Ñ‡',
                r'Ð¼ÐµÐ´Ð²ÐµÐ´|Ð¼Ð¸ÑˆÐº',
                r'Ð»Ð¸Ñ|Ð»Ð¸Ñ†',
                r'Ð·Ð°ÑÑ†|ÐºÑ€Ð¾Ð»Ð¸Ðº',
                r'Ð¼Ñ‹Ñˆ|ÐºÑ€Ñ‹Ñ',
                r'Ð¿Ñ‚Ð¸Ñ†|Ð¿ÐµÑ‚ÑƒÑ…|ÐºÑƒÑ€Ð¸Ñ†Ð°|Ð³ÑƒÑ|ÑƒÑ‚Ðº|Ð²Ð¾Ñ€Ð¾Ð±ÐµÐ¹|Ð²Ð¾Ñ€Ð¾Ð½|Ð¾Ñ€ÐµÐ»|ÑÐ¾Ñ€Ð¾Ðº',
                r'Ñ€Ñ‹Ð±|ÐºÐ°Ñ€Ð°ÑÑŒ|Ñ‰ÑƒÐº|Ð¾ÐºÑƒÐ½',
                r'Ð·Ð¼ÐµÑ|Ð³Ð°Ð´ÑŽÐº|ÑƒÐ¶',
                r'ÑÐ²Ð¸Ð½ÑŒ|Ð¿Ð¾Ñ€Ð¾ÑÐµÐ½',
                r'ÐºÐ¾Ð·ÐµÐ»|ÐºÐ¾Ð·Ð°|Ð±Ð°Ñ€Ð°Ð½|Ð¾Ð²Ñ†',
                r'ÑÐ»Ð¾Ð½|Ñ‚Ð¸Ð³Ñ€|Ð»ÐµÐ²',
                r'Ð±Ð»Ð¾Ñ…|ÐºÐ¾Ð¼Ð°Ñ€|Ð¼ÑƒÑ…Ð°'
            ]
        },
        'money_wealth': {
            'meaning_indicators': [
                'Ð±Ð¾Ð³Ð°Ñ‚ÑÑ‚Ð²Ð¾',
                'Ð±ÐµÐ´Ð½Ð¾ÑÑ‚ÑŒ',
                'Ð´ÐµÐ½ÑŒÐ³Ð¸',
                'Ð¼Ð°Ñ‚ÐµÑ€Ð¸Ð°Ð»ÑŒÐ½Ð¾Ðµ Ð¿Ð¾Ð»Ð¾Ð¶ÐµÐ½Ð¸Ðµ',
                'Ð½Ð¸Ñ‰ÐµÑ‚Ð°',
                'Ð´Ð¾ÑÑ‚Ð°Ñ‚Ð¾Ðº',
                'ÑÐ¾ÑÑ‚Ð¾ÑÐ½Ð¸Ðµ',
                'Ñ„Ð¸Ð½Ð°Ð½ÑÑ‹',
                'Ð±Ð¾Ð³Ð°Ñ‚Ñ‹Ð¹',
                'Ð±ÐµÐ´Ð½Ñ‹Ð¹',
                'Ð½Ð¸Ñ‰Ð¸Ð¹'
            ],
            'patterns': [
                r'Ð±Ð¾Ð³Ð°Ñ‚|ÑÐ¾ÑÑ‚Ð¾ÑÑ‚ÐµÐ»ÑŒÐ½',
                r'Ð±ÐµÐ´ÐµÐ½|Ð½Ð¸Ñ‰|Ð±ÐµÐ´Ð½Ð¾ÑÑ‚',
                r'Ð´ÐµÐ½ÑŒÐ³Ð¸|ÐºÐ°Ð¿Ð¸Ñ‚Ð°Ð»|ÑÑ€ÐµÐ´ÑÑ‚Ð²Ð°',
                r'Ð·Ð¾Ð»Ð¾Ñ‚.*Ð±Ð¾Ð³Ð°Ñ‚|Ð±Ð¾Ð³Ð°Ñ‚.*Ð·Ð¾Ð»Ð¾Ñ‚',
                r'Ð³Ñ€Ð¾Ñˆ|ÐºÐ¾Ð¿ÐµÐ¹Ðº|Ñ€ÑƒÐ±Ð»|Ð¼Ð¾Ð½ÐµÑ‚',
                r'ÐºÐ»Ð°Ð´|ÑÐ¾ÐºÑ€Ð¾Ð²Ð¸Ñ‰',
                r'Ð´Ð¾Ð»Ð³|Ð·Ð°ÐµÐ¼|ÐºÑ€ÐµÐ´Ð¸Ñ‚'
            ]
        },
        'work_labor': {
            'meaning_indicators': [
                'Ñ€Ð°Ð±Ð¾Ñ‚Ð°',
                'Ñ‚Ñ€ÑƒÐ´',
                'Ð´ÐµÑÑ‚ÐµÐ»ÑŒÐ½Ð¾ÑÑ‚ÑŒ',
                'Ð¿Ñ€Ð¾Ñ„ÐµÑÑÐ¸Ñ',
                'Ñ€ÐµÐ¼ÐµÑÐ»Ð¾',
                'ÑÐ»ÑƒÐ¶Ð±Ð°',
                'Ð´ÐµÐ»Ð¾',
                'Ð·Ð°Ð½ÑÑ‚Ð¸Ðµ'
            ],
            'patterns': [
                r'Ñ€Ð°Ð±Ð¾Ñ‚|Ñ‚Ñ€ÑƒÐ´|Ð´ÐµÐ»',
                r'ÑÐ»ÑƒÐ¶Ð±|ÑÐ»ÑƒÐ¶Ð¸Ñ‚ÑŒ',
                r'Ñ€ÐµÐ¼ÐµÑÐ»|Ð¼Ð°ÑÑ‚ÐµÑ€|ÐºÑƒÐ·Ð½ÐµÑ†|ÑÑ‚Ð¾Ð»ÑÑ€',
                r'Ð¿Ð°Ñ…Ð°Ñ‚ÑŒ|ÑÐµÑÑ‚ÑŒ|ÐºÐ¾ÑÐ¸Ñ‚ÑŒ|Ð¶Ð°Ñ‚ÑŒ',
                r'Ð±ÐµÐ·Ð´ÐµÐ»ÑŒÐ½|Ð»ÐµÐ½Ñ‚ÑÐ¹|Ð»ÐµÐ½Ð¸Ñ‚ÑŒÑÑ'
            ]
        },
        'character_behavior': {
            'meaning_indicators': [
                'Ñ…Ð°Ñ€Ð°ÐºÑ‚ÐµÑ€',
                'Ð¿Ð¾Ð²ÐµÐ´ÐµÐ½Ð¸Ðµ',
                'Ð½Ñ€Ð°Ð²',
                'ÐºÐ°Ñ‡ÐµÑÑ‚Ð²Ð¾ Ð»Ð¸Ñ‡Ð½Ð¾ÑÑ‚Ð¸',
                'Ñ‡ÐµÑ€Ñ‚Ð° Ñ…Ð°Ñ€Ð°ÐºÑ‚ÐµÑ€Ð°',
                'Ð¿Ð¾ÑÑ‚ÑƒÐ¿Ð¾Ðº',
                'Ð¼Ð°Ð½ÐµÑ€Ð° Ð¿Ð¾Ð²ÐµÐ´ÐµÐ½Ð¸Ñ'
            ],
            'patterns': [
                r'Ñ…Ð°Ñ€Ð°ÐºÑ‚ÐµÑ€|Ð½Ñ€Ð°Ð²|Ð½Ð°Ñ‚ÑƒÑ€',
                r'Ð´Ð¾Ð±Ñ€|Ð·Ð»|Ñ…Ð¾Ñ€Ð¾Ñˆ|Ð¿Ð»Ð¾Ñ…',
                r'Ñ‡ÐµÑÑ‚Ð½|Ð»Ð¶Ð¸Ð²|Ð¾Ð±Ð¼Ð°Ð½',
                r'Ñ…Ñ€Ð°Ð±Ñ€|Ñ‚Ñ€ÑƒÑÐ»|ÑÐ¼ÐµÐ»',
                r'Ð³Ð¾Ñ€Ð´|ÑÐºÑ€Ð¾Ð¼Ð½|Ñ…Ð²Ð°ÑÑ‚Ð»Ð¸Ð²',
                r'Ð¶Ð°Ð´Ð½|Ñ‰ÐµÐ´Ñ€|ÑÐºÑƒÐ¿Ð¾Ð¹',
                r'Ð»ÐµÐ½Ð¸Ð²Ð¾|Ñ‚Ñ€ÑƒÐ´Ð¾Ð»ÑŽÐ±Ð¸Ð²',
                r'ÑƒÐ¼Ð½Ñ‹Ð¹|Ð³Ð»ÑƒÐ¿Ñ‹Ð¹|Ð´ÑƒÑ€Ð°Ðº'
            ]
        },
        'speech_communication': {
            'meaning_indicators': [
                'Ñ€ÐµÑ‡ÑŒ',
                'Ð¾Ð±Ñ‰ÐµÐ½Ð¸Ðµ',
                'Ñ€Ð°Ð·Ð³Ð¾Ð²Ð¾Ñ€',
                'ÑÐ»Ð¾Ð²Ð°',
                'ÑÐ·Ñ‹Ðº',
                'Ð±ÐµÑÐµÐ´Ð°',
                'Ð¼Ð¾Ð»Ñ‡Ð°Ð½Ð¸Ðµ'
            ],
            'patterns': [
                r'Ð³Ð¾Ð²Ð¾Ñ€|ÑÐºÐ°Ð·Ð°Ñ‚ÑŒ|Ñ€ÐµÑ‡ÑŒ',
                r'ÑÐ»Ð¾Ð²|ÑÐ·Ñ‹Ðº|Ð±Ð¾Ð»Ñ‚Ð°Ñ‚ÑŒ',
                r'Ð¼Ð¾Ð»Ñ‡Ð°Ñ‚ÑŒ|Ð±ÐµÐ·Ð¼Ð¾Ð»Ð²',
                r'ÐºÑ€Ð¸Ñ‡Ð°Ñ‚ÑŒ|ÑˆÐµÐ¿Ñ‚Ð°Ñ‚ÑŒ',
                r'Ñ€Ð°Ð·Ð³Ð¾Ð²Ð¾Ñ€|Ð±ÐµÑÐµÐ´Ð°|ÑÐ¿Ð¾Ñ€',
                r'Ñ€ÑƒÐ³Ð°Ñ‚ÑŒ|Ñ…Ð²Ð°Ð»Ð¸Ñ‚ÑŒ|Ð±Ñ€Ð°Ð½Ð¸Ñ‚ÑŒ'
            ]
        },
        'time_age': {
            'meaning_indicators': [
                'Ð²Ñ€ÐµÐ¼Ñ',
                'Ð²Ð¾Ð·Ñ€Ð°ÑÑ‚',
                'Ð¿ÐµÑ€Ð¸Ð¾Ð´',
                'ÑÐ¿Ð¾Ñ…Ð°',
                'Ð¼Ð¾Ð»Ð¾Ð´Ð¾ÑÑ‚ÑŒ',
                'ÑÑ‚Ð°Ñ€Ð¾ÑÑ‚ÑŒ',
                'Ð´ÐµÑ‚ÑÑ‚Ð²Ð¾'
            ],
            'patterns': [
                r'Ð²Ñ€ÐµÐ¼Ñ|Ñ‡Ð°Ñ|Ð¼Ð¸Ð½ÑƒÑ‚',
                r'Ð²Ð¾Ð·Ñ€Ð°ÑÑ‚|Ð»ÐµÑ‚|Ð³Ð¾Ð´',
                r'Ð¼Ð¾Ð»Ð¾Ð´|ÑÑ‚Ð°Ñ€|Ð´ÐµÑ‚ÑÑ‚Ð²|ÑŽÐ½Ð¾ÑÑ‚',
                r'Ð´ÐµÐ½ÑŒ|Ð½Ð¾Ñ‡ÑŒ|ÑƒÑ‚Ñ€Ð¾|Ð²ÐµÑ‡ÐµÑ€',
                r'ÑÐµÐ·Ð¾Ð½|Ð·Ð¸Ð¼Ð°|Ð»ÐµÑ‚Ð¾|Ð²ÐµÑÐ½Ð°|Ð¾ÑÐµÐ½ÑŒ'
            ]
        },
        'religion_mythology': {
            'meaning_indicators': [
                'Ñ€ÐµÐ»Ð¸Ð³Ð¸Ñ',
                'Ð²ÐµÑ€Ð°',
                'Ñ†ÐµÑ€ÐºÐ¾Ð²ÑŒ',
                'Ð¼Ð¸Ñ„Ð¾Ð»Ð¾Ð³Ð¸Ñ',
                'Ð´Ñ€ÐµÐ²Ð½ÑÑ Ð»ÐµÐ³ÐµÐ½Ð´Ð°',
                'Ð±Ð¸Ð±Ð»Ð¸Ñ',
                'Ð°Ð½Ñ‚Ð¸Ñ‡Ð½Ð¾ÑÑ‚ÑŒ'
            ],
            'patterns': [
                r'Ð±Ð¾Ð³|Ð³Ð¾ÑÐ¿Ð¾Ð´ÑŒ|Ñ…Ñ€Ð¸ÑÑ‚Ð¾Ñ',
                r'Ñ‡ÐµÑ€Ñ‚|Ð´ÑŒÑÐ²Ð¾Ð»|ÑÐ°Ñ‚Ð°Ð½Ð°',
                r'Ð°Ð´|Ñ€Ð°Ð¹|Ð½ÐµÐ±ÐµÑÐ°',
                r'Ð°Ð½Ð³ÐµÐ»|ÑÐ²ÑÑ‚Ð¾Ð¹|Ð³Ñ€ÐµÑ…',
                r'Ñ†ÐµÑ€ÐºÐ¾Ð²|Ð¼Ð¾Ð½Ð°ÑÑ‚Ñ‹Ñ€|Ñ…Ñ€Ð°Ð¼',
                r'Ð¼Ð¾Ð»Ð¸Ñ‚Ð²|ÑÐ»ÑƒÐ¶Ð±.*Ñ†ÐµÑ€ÐºÐ¾Ð²Ð½',
                r'Ð±Ð¸Ð±Ð»|ÐµÐ²Ð°Ð½Ð³',
                r'Ð°Ð½Ñ‚Ð¸Ñ‡Ð½|Ð³Ñ€ÐµÑ‡ÐµÑÐº.*Ð¼Ð¸Ñ„Ð¾Ð»Ð¾Ð³|Ñ€Ð¸Ð¼ÑÐº.*Ð¼Ð¸Ñ„Ð¾Ð»Ð¾Ð³',
                r'Ð³ÐµÑ€Ð°ÐºÐ»|Ð°Ñ…Ð¸Ð»Ð»|Ð°Ð²Ð³Ð¸ÐµÐ²|Ð¾Ð»Ð¸Ð¼Ð¿',
                r'Ð¼Ð¸Ñ„Ð¾Ð»Ð¾Ð³|Ð»ÐµÐ³ÐµÐ½Ð´.*Ð´Ñ€ÐµÐ²Ð½'
            ]
        },
        'weather_nature': {
            'meaning_indicators': [
                'Ð¿Ð¾Ð³Ð¾Ð´Ð°',
                'Ð¿Ñ€Ð¸Ñ€Ð¾Ð´Ð°',
                'ÐºÐ»Ð¸Ð¼Ð°Ñ‚',
                'ÑÑ‚Ð¸Ñ…Ð¸Ñ',
                'Ð¿Ñ€Ð¸Ñ€Ð¾Ð´Ð½Ð¾Ðµ ÑÐ²Ð»ÐµÐ½Ð¸Ðµ'
            ],
            'patterns': [
                r'Ð´Ð¾Ð¶Ð´ÑŒ|ÑÐ½ÐµÐ³|Ð³Ñ€Ð°Ð´',
                r'Ð²ÐµÑ‚ÐµÑ€|Ð±ÑƒÑ€Ñ|ÑƒÑ€Ð°Ð³Ð°Ð½|Ð²Ð¸Ñ…Ñ€ÑŒ',
                r'ÑÐ¾Ð»Ð½Ñ†Ðµ|Ð»ÑƒÐ½Ð°|Ð·Ð²ÐµÐ·Ð´',
                r'Ñ‚ÑƒÐ¼Ð°Ð½|Ð¾Ð±Ð»Ð°Ðº|Ñ‚ÑƒÑ‡Ð¸',
                r'Ñ…Ð¾Ð»Ð¾Ð´|Ð¼Ð¾Ñ€Ð¾Ð·|Ð¶Ð°Ñ€Ð°|Ñ‚ÐµÐ¿Ð»Ð¾',
                r'Ð¼Ð¾Ñ€Ðµ|Ñ€ÐµÐºÐ°|Ð¾Ð·ÐµÑ€Ð¾|Ð²Ð¾Ð´Ð°',
                r'Ð»ÐµÑ|Ð¿Ð¾Ð»Ðµ|Ð³Ð¾Ñ€Ð°|Ð·ÐµÐ¼Ð»Ñ',
                r'Ð¿Ð¾Ð³Ð¾Ð´|ÐºÐ»Ð¸Ð¼Ð°Ñ‚|ÑÑ‚Ð¸Ñ…Ð¸Ñ'
            ]
        },
        'mind_intelligence': {
            'meaning_indicators': [
                'ÑƒÐ¼',
                'Ð¸Ð½Ñ‚ÐµÐ»Ð»ÐµÐºÑ‚',
                'Ñ€Ð°Ð·ÑƒÐ¼',
                'Ð³Ð»ÑƒÐ¿Ð¾ÑÑ‚ÑŒ',
                'Ð¼Ñ‹ÑˆÐ»ÐµÐ½Ð¸Ðµ',
                'Ð¿Ð°Ð¼ÑÑ‚ÑŒ',
                'Ð·Ð½Ð°Ð½Ð¸Ðµ'
            ],
            'patterns': [
                r'ÑƒÐ¼|ÑƒÐ¼Ð½Ñ‹Ð¹|ÑƒÐ¼ÐµÐ½',
                r'Ð³Ð»ÑƒÐ¿|Ð´ÑƒÑ€Ð°Ðº|Ð´ÑƒÑ€Ð°|Ð³Ð»ÑƒÐ¿Ð¾ÑÑ‚',
                r'Ð¼ÑƒÐ´Ñ€|Ð¼ÑƒÐ´Ñ€Ð¾ÑÑ‚',
                r'Ñ€Ð°Ð·ÑƒÐ¼|Ñ€Ð°ÑÑÑƒÐ´Ð¾Ðº',
                r'Ð¿Ð°Ð¼ÑÑ‚ÑŒ|Ð¿Ð¾Ð¼Ð½Ð¸Ñ‚ÑŒ|Ð·Ð°Ð±Ñ‹Ð²',
                r'Ð´ÑƒÐ¼Ð°Ñ‚ÑŒ|Ð¼Ñ‹ÑÐ»|ÑÐ¾Ð¾Ð±Ñ€Ð°Ð¶',
                r'Ð·Ð½Ð°Ñ‚ÑŒ|Ð¿Ð¾Ð½Ð¸Ð¼Ð°Ñ‚ÑŒ|ÑƒÑ‡Ð¸Ñ‚ÑŒ',
                r'Ð¼Ð¾Ð·Ð³|Ð³Ð¾Ð»Ð¾Ð².*ÑƒÐ¼'
            ]
        }
    }
    
    # Analyze each phrase
    for i, phrase_data in enumerate(phrases):
        phrase = phrase_data['phrase'].lower()
        meanings = ' '.join(phrase_data.get('meanings', [])).lower()
        etymology = phrase_data.get('etymology', '').lower()
        current_category = phrase_data['category']
        
        # Combine all text for analysis
        full_text = f"{phrase} {meanings} {etymology}"
        
        # Find the best matching category based on semantic meaning
        best_category = None
        best_score = 0
        
        for category, rules in semantic_rules.items():
            score = 0
            
            # Check meaning indicators (high weight)
            if 'meaning_indicators' in rules:
                for indicator in rules['meaning_indicators']:
                    if indicator in meanings:
                        score += 5
            
            # Check patterns in the full text
            if 'patterns' in rules:
                for pattern in rules['patterns']:
                    if re.search(pattern, full_text):
                        score += 2
            
            # Special case: if phrase is about a physical body part action but meaning is metaphorical
            if category == 'body_parts':
                # Check if the meaning is actually about the body part itself
                body_part_meanings = [
                    'Ñ„Ð¸Ð·Ð¸Ñ‡ÐµÑÐºÐ¾Ðµ',
                    'Ð°Ð½Ð°Ñ‚Ð¾Ð¼Ð¸Ñ',
                    'Ñ‚ÐµÐ»ÐµÑÐ½Ñ‹Ð¹',
                    'Ð¾Ñ€Ð³Ð°Ð½',
                    'Ñ‡Ð°ÑÑ‚ÑŒ Ñ‚ÐµÐ»Ð°'
                ]
                if not any(bp in meanings for bp in body_part_meanings):
                    # If body part is mentioned but meaning is metaphorical, reduce score
                    if score > 0:
                        score = max(1, score - 3)
            
            if score > best_score:
                best_score = score
                best_category = category
        
        # If we found a better category with good confidence
        if best_category and best_category != current_category and best_score >= 3:
            corrections.append({
                'index': i,
                'phrase': phrase_data['phrase'],
                'current_category': current_category,
                'suggested_category': best_category,
                'score': best_score,
                'meaning': phrase_data.get('meanings', [''])[0] if phrase_data.get('meanings') else '',
                'reason': f'Semantic analysis (score: {best_score})'
            })
    
    return corrections

def apply_semantic_corrections(data, corrections, apply_threshold=5):
    """Apply semantic corrections to the data."""
    phrases = data['phrases']
    
    applied_count = 0
    for correction in corrections:
        if correction['score'] >= apply_threshold:
            phrase_data = phrases[correction['index']]
            old_category = phrase_data['category']
            phrase_data['category'] = correction['suggested_category']
            print(f"âœ… '{correction['phrase']}' {old_category} â†’ {correction['suggested_category']} (score: {correction['score']})")
            applied_count += 1
    
    print(f"\nðŸ“Š Applied {applied_count} semantic corrections")
    return data

def main():
    """Main function to analyze and fix semantic categorization."""
    print("ðŸ”§ Starting semantic categorization analysis...")
    
    # Load data
    data = load_phrases()
    
    # Analyze semantic issues
    corrections = analyze_semantic_categorization(data)
    
    print(f"\nðŸš¨ Found {len(corrections)} potential semantic corrections")
    
    # Show top corrections
    print(f"\nðŸ“‹ Top 30 semantic corrections:")
    for correction in sorted(corrections, key=lambda x: x['score'], reverse=True)[:30]:
        print(f"  '{correction['phrase']}' - {correction['current_category']} â†’ {correction['suggested_category']} (score: {correction['score']})")
        print(f"     Meaning: {correction['meaning'][:100]}{'...' if len(correction['meaning']) > 100 else ''}")
        print()
    
    # Apply high-confidence corrections
    print(f"\nðŸ”„ Applying high-confidence corrections (score >= 5)...")
    fixed_data = apply_semantic_corrections(data, corrections, apply_threshold=5)
    
    # Save corrected data
    with open('table_phrases_semantic_fixed.json', 'w', encoding='utf-8') as f:
        json.dump(fixed_data, f, ensure_ascii=False, indent=2)
    
    print(f"\nðŸ’¾ Saved semantically corrected data to table_phrases_semantic_fixed.json")
    
    return corrections

if __name__ == "__main__":
    corrections = main()