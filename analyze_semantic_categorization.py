#!/usr/bin/env python3
"""
Script to analyze phraseological units and correct categorization based on semantic meaning.
This script focuses on the actual meaning of the complete expression, not individual words.
"""

import json
import re
from collections import defaultdict

def load_phrases():
    """Load phrases from the JSON file."""
    with open('table_phrases.json', 'r', encoding='utf-8') as f:
        data = json.load(f)
    return data

def analyze_semantic_categorization(data):
    """Analyze and suggest corrections based on semantic meaning."""
    phrases = data['phrases']
    categories = data['categories']
    
    corrections = []
    
    print("üîç Analyzing semantic categorization...")
    print(f"Total phrases: {len(phrases)}")
    
    # Define semantic categorization rules based on meaning, not keywords
    semantic_rules = {
        'emotions_feelings': {
            'patterns': [
                r'—Ä–∞–¥–æ—Å—Ç|—Å—á–∞—Å—Ç–ª–∏–≤|–≤–µ—Å–µ–ª|—Å–º–µ—Ö|—É–ª—ã–±–∫',
                r'–≥—Ä—É—Å—Ç—å|–ø–µ—á–∞–ª—å|–≥–æ—Ä–µ|—Å–ª–µ–∑|–ø–ª–∞—á|—Ç–æ—Å–∫–∞',
                r'—Å—Ç—Ä–∞—Ö|–±–æ—è–∑–Ω|–∏—Å–ø—É–≥|—É–∂–∞—Å|—Ç—Ä—É—Å',
                r'–≥–Ω–µ–≤|–∑–ª–æ—Å—Ç|—è—Ä–æ—Å—Ç|—Å–µ—Ä–¥–∏—Ç',
                r'–ª—é–±–æ–≤—å|–≤–ª—é–±–ª|—Å—Ç—Ä–∞—Å—Ç',
                r'–Ω–µ–Ω–∞–≤–∏—Å—Ç|–≤—Ä–∞–∂–¥–µ–±',
                r'–≤–æ–ª–Ω–µ–Ω–∏–µ|–±–µ—Å–ø–æ–∫–æ–π|—Ç—Ä–µ–≤–æ–≥',
                r'—Å—Ç—ã–¥|—Å—Ä–∞–º|–ø–æ–∑–æ—Ä',
                r'–∑–∞–≤–∏—Å—Ç—å|—Ä–µ–≤–Ω–æ—Å—Ç',
                r'—É–¥–∏–≤–ª–µ–Ω–∏–µ|–∏–∑—É–º–ª–µ–Ω–∏–µ',
                r'—ç–º–æ—Ü–∏|—á—É–≤—Å—Ç–≤|–Ω–∞—Å—Ç—Ä–æ–µ–Ω–∏–µ'
            ],
            'meaning_indicators': [
                '—ç–º–æ—Ü–∏–æ–Ω–∞–ª—å–Ω–æ–µ —Å–æ—Å—Ç–æ—è–Ω–∏–µ',
                '—á—É–≤—Å—Ç–≤–æ',
                '–ø–µ—Ä–µ–∂–∏–≤–∞–Ω–∏–µ',
                '–Ω–∞—Å—Ç—Ä–æ–µ–Ω–∏–µ',
                '–¥—É—à–µ–≤–Ω–æ–µ –≤–æ–ª–Ω–µ–Ω–∏–µ'
            ]
        },
        'body_parts': {
            'patterns': [
                r'–≥–æ–ª–æ–≤|–±–∞—à–∫|—á–µ—Ä–µ–ø',
                r'–≥–ª–∞–∑|–æ–∫–æ|–≤–∑–≥–ª—è–¥|–≤–∑–æ—Ä',
                r'—Ä—É–∫|–ª–∞–¥–æ–Ω|–ø–∞–ª—å—Ü|–∫—É–ª–∞–∫',
                r'–Ω–æ–≥|—Å—Ç–æ–ø|–ø—è—Ç',
                r'—Å–µ—Ä–¥—Ü|–¥—É—à',
                r'—è–∑—ã–∫|—Ä–æ—Ç|–≥—É–±|–∑—É–±',
                r'—É—Ö–æ|—Å–ª—É—Ö',
                r'–Ω–æ—Å|–æ–±–æ–Ω—è–Ω–∏–µ',
                r'–ª–∏—Ü–æ|–ª–∏–∫|—â–µ–∫',
                r'—Å–ø–∏–Ω|–ø–ª–µ—á|–≥—Ä—É–¥—å',
                r'–∂–∏–≤–æ—Ç|–∂–µ–ª—É–¥–æ–∫',
                r'—à–µ—è|–≥–æ—Ä–ª–æ',
                r'–≤–æ–ª–æ—Å|–±–æ—Ä–æ–¥–∞',
                r'–∫–æ–∂–∞|—Ç–µ–ª–æ'
            ]
        },
        'animals': {
            'patterns': [
                r'–∫–æ—Ç|–∫–æ—à–∫|–∫–æ—Ç–µ–Ω',
                r'—Å–æ–±–∞–∫|–ø–µ—Å|—â–µ–Ω–æ–∫',
                r'–ª–æ—à–∞–¥|–∫–æ–Ω—å|–∫–æ–±—ã–ª|–∂–µ—Ä–µ–±–µ—Ü',
                r'–∫–æ—Ä–æ–≤–∞|–±—ã–∫|—Ç–µ–ª–µ–Ω',
                r'–≤–æ–ª–∫|–≤–æ–ª—á',
                r'–º–µ–¥–≤–µ–¥|–º–∏—à–∫',
                r'–ª–∏—Å|–ª–∏—Ü',
                r'–∑–∞—è—Ü|–∫—Ä–æ–ª–∏–∫',
                r'–º—ã—à|–∫—Ä—ã—Å',
                r'–ø—Ç–∏—Ü|–ø–µ—Ç—É—Ö|–∫—É—Ä–∏—Ü–∞|–≥—É—Å|—É—Ç–∫|–≤–æ—Ä–æ–±–µ–π|–≤–æ—Ä–æ–Ω|–æ—Ä–µ–ª|—Å–æ—Ä–æ–∫',
                r'—Ä—ã–±|–∫–∞—Ä–∞—Å—å|—â—É–∫|–æ–∫—É–Ω',
                r'–∑–º–µ—è|–≥–∞–¥—é–∫|—É–∂',
                r'—Å–≤–∏–Ω—å|–ø–æ—Ä–æ—Å–µ–Ω',
                r'–∫–æ–∑–µ–ª|–∫–æ–∑–∞|–±–∞—Ä–∞–Ω|–æ–≤—Ü',
                r'—Å–ª–æ–Ω|—Ç–∏–≥—Ä|–ª–µ–≤',
                r'–±–ª–æ—Ö|–∫–æ–º–∞—Ä|–º—É—Ö–∞'
            ]
        },
        'money_wealth': {
            'meaning_indicators': [
                '–±–æ–≥–∞—Ç—Å—Ç–≤–æ',
                '–±–µ–¥–Ω–æ—Å—Ç—å',
                '–¥–µ–Ω—å–≥–∏',
                '–º–∞—Ç–µ—Ä–∏–∞–ª—å–Ω–æ–µ –ø–æ–ª–æ–∂–µ–Ω–∏–µ',
                '–Ω–∏—â–µ—Ç–∞',
                '–¥–æ—Å—Ç–∞—Ç–æ–∫',
                '—Å–æ—Å—Ç–æ—è–Ω–∏–µ',
                '—Ñ–∏–Ω–∞–Ω—Å—ã',
                '–±–æ–≥–∞—Ç—ã–π',
                '–±–µ–¥–Ω—ã–π',
                '–Ω–∏—â–∏–π'
            ],
            'patterns': [
                r'–±–æ–≥–∞—Ç|—Å–æ—Å—Ç–æ—è—Ç–µ–ª—å–Ω',
                r'–±–µ–¥–µ–Ω|–Ω–∏—â|–±–µ–¥–Ω–æ—Å—Ç',
                r'–¥–µ–Ω—å–≥–∏|–∫–∞–ø–∏—Ç–∞–ª|—Å—Ä–µ–¥—Å—Ç–≤–∞',
                r'–∑–æ–ª–æ—Ç.*–±–æ–≥–∞—Ç|–±–æ–≥–∞—Ç.*–∑–æ–ª–æ—Ç',
                r'–≥—Ä–æ—à|–∫–æ–ø–µ–π–∫|—Ä—É–±–ª|–º–æ–Ω–µ—Ç',
                r'–∫–ª–∞–¥|—Å–æ–∫—Ä–æ–≤–∏—â',
                r'–¥–æ–ª–≥|–∑–∞–µ–º|–∫—Ä–µ–¥–∏—Ç'
            ]
        },
        'work_labor': {
            'meaning_indicators': [
                '—Ä–∞–±–æ—Ç–∞',
                '—Ç—Ä—É–¥',
                '–¥–µ—è—Ç–µ–ª—å–Ω–æ—Å—Ç—å',
                '–ø—Ä–æ—Ñ–µ—Å—Å–∏—è',
                '—Ä–µ–º–µ—Å–ª–æ',
                '—Å–ª—É–∂–±–∞',
                '–¥–µ–ª–æ',
                '–∑–∞–Ω—è—Ç–∏–µ'
            ],
            'patterns': [
                r'—Ä–∞–±–æ—Ç|—Ç—Ä—É–¥|–¥–µ–ª',
                r'—Å–ª—É–∂–±|—Å–ª—É–∂–∏—Ç—å',
                r'—Ä–µ–º–µ—Å–ª|–º–∞—Å—Ç–µ—Ä|–∫—É–∑–Ω–µ—Ü|—Å—Ç–æ–ª—è—Ä',
                r'–ø–∞—Ö–∞—Ç—å|—Å–µ—è—Ç—å|–∫–æ—Å–∏—Ç—å|–∂–∞—Ç—å',
                r'–±–µ–∑–¥–µ–ª—å–Ω|–ª–µ–Ω—Ç—è–π|–ª–µ–Ω–∏—Ç—å—Å—è'
            ]
        },
        'character_behavior': {
            'meaning_indicators': [
                '—Ö–∞—Ä–∞–∫—Ç–µ—Ä',
                '–ø–æ–≤–µ–¥–µ–Ω–∏–µ',
                '–Ω—Ä–∞–≤',
                '–∫–∞—á–µ—Å—Ç–≤–æ –ª–∏—á–Ω–æ—Å—Ç–∏',
                '—á–µ—Ä—Ç–∞ —Ö–∞—Ä–∞–∫—Ç–µ—Ä–∞',
                '–ø–æ—Å—Ç—É–ø–æ–∫',
                '–º–∞–Ω–µ—Ä–∞ –ø–æ–≤–µ–¥–µ–Ω–∏—è'
            ],
            'patterns': [
                r'—Ö–∞—Ä–∞–∫—Ç–µ—Ä|–Ω—Ä–∞–≤|–Ω–∞—Ç—É—Ä',
                r'–¥–æ–±—Ä|–∑–ª|—Ö–æ—Ä–æ—à|–ø–ª–æ—Ö',
                r'—á–µ—Å—Ç–Ω|–ª–∂–∏–≤|–æ–±–º–∞–Ω',
                r'—Ö—Ä–∞–±—Ä|—Ç—Ä—É—Å–ª|—Å–º–µ–ª',
                r'–≥–æ—Ä–¥|—Å–∫—Ä–æ–º–Ω|—Ö–≤–∞—Å—Ç–ª–∏–≤',
                r'–∂–∞–¥–Ω|—â–µ–¥—Ä|—Å–∫—É–ø–æ–π',
                r'–ª–µ–Ω–∏–≤–æ|—Ç—Ä—É–¥–æ–ª—é–±–∏–≤',
                r'—É–º–Ω—ã–π|–≥–ª—É–ø—ã–π|–¥—É—Ä–∞–∫'
            ]
        },
        'speech_communication': {
            'meaning_indicators': [
                '—Ä–µ—á—å',
                '–æ–±—â–µ–Ω–∏–µ',
                '—Ä–∞–∑–≥–æ–≤–æ—Ä',
                '—Å–ª–æ–≤–∞',
                '—è–∑—ã–∫',
                '–±–µ—Å–µ–¥–∞',
                '–º–æ–ª—á–∞–Ω–∏–µ'
            ],
            'patterns': [
                r'–≥–æ–≤–æ—Ä|—Å–∫–∞–∑–∞—Ç—å|—Ä–µ—á—å',
                r'—Å–ª–æ–≤|—è–∑—ã–∫|–±–æ–ª—Ç–∞—Ç—å',
                r'–º–æ–ª—á–∞—Ç—å|–±–µ–∑–º–æ–ª–≤',
                r'–∫—Ä–∏—á–∞—Ç—å|—à–µ–ø—Ç–∞—Ç—å',
                r'—Ä–∞–∑–≥–æ–≤–æ—Ä|–±–µ—Å–µ–¥–∞|—Å–ø–æ—Ä',
                r'—Ä—É–≥–∞—Ç—å|—Ö–≤–∞–ª–∏—Ç—å|–±—Ä–∞–Ω–∏—Ç—å'
            ]
        },
        'time_age': {
            'meaning_indicators': [
                '–≤—Ä–µ–º—è',
                '–≤–æ–∑—Ä–∞—Å—Ç',
                '–ø–µ—Ä–∏–æ–¥',
                '—ç–ø–æ—Ö–∞',
                '–º–æ–ª–æ–¥–æ—Å—Ç—å',
                '—Å—Ç–∞—Ä–æ—Å—Ç—å',
                '–¥–µ—Ç—Å—Ç–≤–æ'
            ],
            'patterns': [
                r'–≤—Ä–µ–º—è|—á–∞—Å|–º–∏–Ω—É—Ç',
                r'–≤–æ–∑—Ä–∞—Å—Ç|–ª–µ—Ç|–≥–æ–¥',
                r'–º–æ–ª–æ–¥|—Å—Ç–∞—Ä|–¥–µ—Ç—Å—Ç–≤|—é–Ω–æ—Å—Ç',
                r'–¥–µ–Ω—å|–Ω–æ—á—å|—É—Ç—Ä–æ|–≤–µ—á–µ—Ä',
                r'—Å–µ–∑–æ–Ω|–∑–∏–º–∞|–ª–µ—Ç–æ|–≤–µ—Å–Ω–∞|–æ—Å–µ–Ω—å'
            ]
        },
        'religion_mythology': {
            'meaning_indicators': [
                '—Ä–µ–ª–∏–≥–∏—è',
                '–≤–µ—Ä–∞',
                '—Ü–µ—Ä–∫–æ–≤—å',
                '–º–∏—Ñ–æ–ª–æ–≥–∏—è',
                '–¥—Ä–µ–≤–Ω—è—è –ª–µ–≥–µ–Ω–¥–∞',
                '–±–∏–±–ª–∏—è',
                '–∞–Ω—Ç–∏—á–Ω–æ—Å—Ç—å'
            ],
            'patterns': [
                r'–±–æ–≥|–≥–æ—Å–ø–æ–¥—å|—Ö—Ä–∏—Å—Ç–æ—Å',
                r'—á–µ—Ä—Ç|–¥—å—è–≤–æ–ª|—Å–∞—Ç–∞–Ω–∞',
                r'–∞–¥|—Ä–∞–π|–Ω–µ–±–µ—Å–∞',
                r'–∞–Ω–≥–µ–ª|—Å–≤—è—Ç–æ–π|–≥—Ä–µ—Ö',
                r'—Ü–µ—Ä–∫–æ–≤|–º–æ–Ω–∞—Å—Ç—ã—Ä|—Ö—Ä–∞–º',
                r'–º–æ–ª–∏—Ç–≤|—Å–ª—É–∂–±.*—Ü–µ—Ä–∫–æ–≤–Ω',
                r'–±–∏–±–ª|–µ–≤–∞–Ω–≥',
                r'–∞–Ω—Ç–∏—á–Ω|–≥—Ä–µ—á–µ—Å–∫.*–º–∏—Ñ–æ–ª–æ–≥|—Ä–∏–º—Å–∫.*–º–∏—Ñ–æ–ª–æ–≥',
                r'–≥–µ—Ä–∞–∫–ª|–∞—Ö–∏–ª–ª|–∞–≤–≥–∏–µ–≤|–æ–ª–∏–º–ø',
                r'–º–∏—Ñ–æ–ª–æ–≥|–ª–µ–≥–µ–Ω–¥.*–¥—Ä–µ–≤–Ω'
            ]
        },
        'weather_nature': {
            'meaning_indicators': [
                '–ø–æ–≥–æ–¥–∞',
                '–ø—Ä–∏—Ä–æ–¥–∞',
                '–∫–ª–∏–º–∞—Ç',
                '—Å—Ç–∏—Ö–∏—è',
                '–ø—Ä–∏—Ä–æ–¥–Ω–æ–µ —è–≤–ª–µ–Ω–∏–µ'
            ],
            'patterns': [
                r'–¥–æ–∂–¥—å|—Å–Ω–µ–≥|–≥—Ä–∞–¥',
                r'–≤–µ—Ç–µ—Ä|–±—É—Ä—è|—É—Ä–∞–≥–∞–Ω|–≤–∏—Ö—Ä—å',
                r'—Å–æ–ª–Ω—Ü–µ|–ª—É–Ω–∞|–∑–≤–µ–∑–¥',
                r'—Ç—É–º–∞–Ω|–æ–±–ª–∞–∫|—Ç—É—á–∏',
                r'—Ö–æ–ª–æ–¥|–º–æ—Ä–æ–∑|–∂–∞—Ä–∞|—Ç–µ–ø–ª–æ',
                r'–º–æ—Ä–µ|—Ä–µ–∫–∞|–æ–∑–µ—Ä–æ|–≤–æ–¥–∞',
                r'–ª–µ—Å|–ø–æ–ª–µ|–≥–æ—Ä–∞|–∑–µ–º–ª—è',
                r'–ø–æ–≥–æ–¥|–∫–ª–∏–º–∞—Ç|—Å—Ç–∏—Ö–∏—è'
            ]
        },
        'mind_intelligence': {
            'meaning_indicators': [
                '—É–º',
                '–∏–Ω—Ç–µ–ª–ª–µ–∫—Ç',
                '—Ä–∞–∑—É–º',
                '–≥–ª—É–ø–æ—Å—Ç—å',
                '–º—ã—à–ª–µ–Ω–∏–µ',
                '–ø–∞–º—è—Ç—å',
                '–∑–Ω–∞–Ω–∏–µ'
            ],
            'patterns': [
                r'—É–º|—É–º–Ω—ã–π|—É–º–µ–Ω',
                r'–≥–ª—É–ø|–¥—É—Ä–∞–∫|–¥—É—Ä–∞|–≥–ª—É–ø–æ—Å—Ç',
                r'–º—É–¥—Ä|–º—É–¥—Ä–æ—Å—Ç',
                r'—Ä–∞–∑—É–º|—Ä–∞—Å—Å—É–¥–æ–∫',
                r'–ø–∞–º—è—Ç—å|–ø–æ–º–Ω–∏—Ç—å|–∑–∞–±—ã–≤',
                r'–¥—É–º–∞—Ç—å|–º—ã—Å–ª|—Å–æ–æ–±—Ä–∞–∂',
                r'–∑–Ω–∞—Ç—å|–ø–æ–Ω–∏–º–∞—Ç—å|—É—á–∏—Ç—å',
                r'–º–æ–∑–≥|–≥–æ–ª–æ–≤.*—É–º'
            ]
        }
    }
    
    # Analyze each phrase
    for i, phrase_data in enumerate(phrases):
        phrase = phrase_data['phrase'].lower()
        meanings = ' '.join(phrase_data.get('meanings', [])).lower()
        etymology = phrase_data.get('etymology', '').lower()
        current_category = phrase_data['category']
        
        # Combine all text for analysis
        full_text = f"{phrase} {meanings} {etymology}"
        
        # Find the best matching category based on semantic meaning
        best_category = None
        best_score = 0
        
        for category, rules in semantic_rules.items():
            score = 0
            
            # Check meaning indicators (high weight)
            if 'meaning_indicators' in rules:
                for indicator in rules['meaning_indicators']:
                    if indicator in meanings:
                        score += 5
            
            # Check patterns in the full text
            if 'patterns' in rules:
                for pattern in rules['patterns']:
                    if re.search(pattern, full_text):
                        score += 2
            
            # Special case: if phrase is about a physical body part action but meaning is metaphorical
            if category == 'body_parts':
                # Check if the meaning is actually about the body part itself
                body_part_meanings = [
                    '—Ñ–∏–∑–∏—á–µ—Å–∫–æ–µ',
                    '–∞–Ω–∞—Ç–æ–º–∏—è',
                    '—Ç–µ–ª–µ—Å–Ω—ã–π',
                    '–æ—Ä–≥–∞–Ω',
                    '—á–∞—Å—Ç—å —Ç–µ–ª–∞'
                ]
                if not any(bp in meanings for bp in body_part_meanings):
                    # If body part is mentioned but meaning is metaphorical, reduce score
                    if score > 0:
                        score = max(1, score - 3)
            
            if score > best_score:
                best_score = score
                best_category = category
        
        # If we found a better category with good confidence
        if best_category and best_category != current_category and best_score >= 3:
            corrections.append({
                'index': i,
                'phrase': phrase_data['phrase'],
                'current_category': current_category,
                'suggested_category': best_category,
                'score': best_score,
                'meaning': phrase_data.get('meanings', [''])[0] if phrase_data.get('meanings') else '',
                'reason': f'Semantic analysis (score: {best_score})'
            })
    
    return corrections

def apply_semantic_corrections(data, corrections, apply_threshold=5):
    """Apply semantic corrections to the data."""
    phrases = data['phrases']
    
    applied_count = 0
    for correction in corrections:
        if correction['score'] >= apply_threshold:
            phrase_data = phrases[correction['index']]
            old_category = phrase_data['category']
            phrase_data['category'] = correction['suggested_category']
            print(f"‚úÖ '{correction['phrase']}' {old_category} ‚Üí {correction['suggested_category']} (score: {correction['score']})")
            applied_count += 1
    
    print(f"\nüìä Applied {applied_count} semantic corrections")
    return data

def main():
    """Main function to analyze and fix semantic categorization."""
    print("üîß Starting semantic categorization analysis...")
    
    # Load data
    data = load_phrases()
    
    # Analyze semantic issues
    corrections = analyze_semantic_categorization(data)
    
    print(f"\nüö® Found {len(corrections)} potential semantic corrections")
    
    # Show top corrections
    print(f"\nüìã Top 30 semantic corrections:")
    for correction in sorted(corrections, key=lambda x: x['score'], reverse=True)[:30]:
        print(f"  '{correction['phrase']}' - {correction['current_category']} ‚Üí {correction['suggested_category']} (score: {correction['score']})")
        print(f"     Meaning: {correction['meaning'][:100]}{'...' if len(correction['meaning']) > 100 else ''}")
        print()
    
    # Apply high-confidence corrections
    print(f"\nüîÑ Applying high-confidence corrections (score >= 5)...")
    fixed_data = apply_semantic_corrections(data, corrections, apply_threshold=5)
    
    # Save corrected data
    with open('table_phrases_semantic_fixed.json', 'w', encoding='utf-8') as f:
        json.dump(fixed_data, f, ensure_ascii=False, indent=2)
    
    print(f"\nüíæ Saved semantically corrected data to table_phrases_semantic_fixed.json")
    
    return corrections

if __name__ == "__main__":
    corrections = main()