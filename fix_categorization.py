#!/usr/bin/env python3
"""
Script to analyze and fix categorization issues in table_phrases.json.
This script identifies phrases that are incorrectly categorized and suggests proper categories.
"""

import json
import re
from collections import defaultdict, Counter

def load_phrases():
    """Load phrases from the JSON file."""
    with open('table_phrases.json', 'r', encoding='utf-8') as f:
        data = json.load(f)
    return data

def analyze_categorization_issues(data):
    """Analyze and identify categorization issues."""
    phrases = data['phrases']
    categories = data['categories']
    
    issues = []
    fixes = []
    
    print("üîç Analyzing categorization issues...")
    print(f"Total phrases: {len(phrases)}")
    
    # Category distribution
    category_counts = Counter(phrase['category'] for phrase in phrases)
    print(f"\nüìä Current category distribution:")
    for cat, count in category_counts.most_common():
        cat_name = categories.get(cat, {}).get('name', cat)
        print(f"  {cat_name} ({cat}): {count} phrases")
    
    # Define more specific categorization rules based on semantic meaning
    categorization_rules = {
        'emotions_feelings': {
            'keywords': ['—Ä–∞–¥–æ—Å—Ç', '–≥—Ä—É—Å—Ç—å', '–ø–µ—á–∞–ª—å', '—Å—Ç—Ä–∞—Ö', '—É–∂–∞—Å', '–≥–Ω–µ–≤', '–∑–ª–æ—Å—Ç', '—è—Ä–æ—Å—Ç', 
                        '—Å—á–∞—Å—Ç—å–µ', '–≥–æ—Ä–µ', '—Ç–æ—Å–∫–∞', '—Å–∫—É–∫–∞', '–≤–µ—Å–µ–ª', '—Å–º–µ—Ö', '—Å–ª–µ–∑—ã', '–ø–ª–∞—á', 
                        '–æ–±–∏–¥–∞', '–∑–∞–≤–∏—Å—Ç—å', '—Ä–µ–≤–Ω–æ—Å—Ç', '–ª—é–±–æ–≤—å', '–Ω–µ–Ω–∞–≤–∏—Å—Ç', '–≤–æ–ª–Ω–µ–Ω–∏–µ', '–±–µ—Å–ø–æ–∫–æ–π',
                        '—Ç—Ä–µ–≤–æ–≥', '–∏—Å–ø—É–≥', '–±–æ—è', '–æ–ø–∞—Å'],
            'semantic_patterns': [
                r'–±–æ—è—Ç—å—Å—è|—Å—Ç—Ä–∞—à–Ω–æ|–∏—Å–ø—É–≥|–ø—É–≥–∞—Ç—å',
                r'—Ä–∞–¥–æ—Å—Ç—å|–≤–µ—Å–µ–ª—å–µ|—Å–º–µ—Ö|—É–ª—ã–±–∫–∞',
                r'–≥—Ä—É—Å—Ç—å|–ø–µ—á–∞–ª—å|–≥–æ—Ä–µ|—Å–ª–µ–∑—ã',
                r'–≥–Ω–µ–≤|–∑–ª–æ—Å—Ç—å|—è—Ä–æ—Å—Ç—å|—Ä–∞–∑–¥—Ä–∞–∂–µ–Ω–∏–µ'
            ]
        },
        'body_parts': {
            'keywords': ['–≥–æ–ª–æ–≤–∞', '–≥–ª–∞–∑', '—É—Ö–æ', '–Ω–æ—Å', '—Ä–æ—Ç', '—è–∑—ã–∫', '–∑—É–±', '—Ä—É–∫–∞', '–Ω–æ–≥–∞', 
                        '–ø–∞–ª—å—Ü', '–ø–ª–µ—á–æ', '—Å–ø–∏–Ω–∞', '–≥—Ä—É–¥—å', '—Å–µ—Ä–¥—Ü–µ', '–¥—É—à–∞', '–∂–∏–≤–æ—Ç', '—à–µ—è', 
                        '–ª–∏—Ü–æ', '–ª–æ–±', '–±–æ—Ä–æ–¥–∞', '–≤–æ–ª–æ—Å', '–∫–æ–∂–∞'],
            'semantic_patterns': [
                r'–≥–æ–ª–æ–≤|–≥–ª–∞–∑|—Ä—É–∫|–Ω–æ–≥|—Å–µ—Ä–¥—Ü|–¥—É—à',
                r'—è–∑—ã–∫|–∑—É–±|–Ω–æ—Å|—É—Ö–æ|–ª–∏—Ü–æ'
            ]
        },
        'animals': {
            'keywords': ['–∫–æ—Ç', '—Å–æ–±–∞–∫', '–≤–æ–ª–∫', '–ª–∏—Å–∞', '–º–µ–¥–≤–µ–¥', '–∑–∞—è—Ü', '–º—ã—à', '–ø—Ç–∏—Ü', '–∫–æ–Ω—å', 
                        '–ª–æ—à–∞–¥', '–∫–æ—Ä–æ–≤–∞', '–æ–≤—Ü', '–∫–æ–∑–µ–ª', '–±–∞—Ä–∞–Ω', '—Ä—ã–±', '–∑–º–µ—è', '–ª–µ–≤', '—Ç–∏–≥—Ä', 
                        '—Å–ª–æ–Ω', '–æ—Ä–µ–ª', '–≤–æ—Ä–æ–Ω', '—Å–æ—Ä–æ–∫', '–≤–æ—Ä–æ–±–µ–π', '–≥—É—Å', '—É—Ç–∫', '–ø–µ—Ç—É—Ö', '–∫—É—Ä–∏—Ü–∞',
                        '—Å–≤–∏–Ω—å', '–∫–æ–±—ã–ª', '–∂–µ—Ä–µ–±–µ—Ü', '—Ç–µ–ª—è—Ç', '–±–ª–æ—Ö'],
            'semantic_patterns': [
                r'–∫–æ—Ç|–∫–æ—à–∫|–∫–æ—Ç—ë–Ω–æ–∫|–∫–æ—Ç–µ–Ω–æ–∫',
                r'—Å–æ–±–∞–∫|–ø—ë—Å|–ø–µ—Å|—â–µ–Ω–æ–∫',
                r'–≤–æ–ª–∫|–≤–æ–ª—á',
                r'–º–µ–¥–≤–µ–¥|–º–∏—à–∫',
                r'—Ä—ã–±|–∫–∞—Ä–∞—Å—å|—â—É–∫',
                r'–ø—Ç–∏—Ü|–ø–µ—Ç—É—Ö|–∫—É—Ä–∏—Ü–∞|–≥—É—Å|—É—Ç–∫|–≤–æ—Ä–æ–±–µ–π|–≤–æ—Ä–æ–Ω|–æ—Ä–µ–ª',
                r'–ª–æ—à–∞–¥|–∫–æ–Ω—å|–∫–æ–±—ã–ª|–∂–µ—Ä–µ–±–µ—Ü|–º–µ—Ä–∏–Ω',
                r'–∫–æ—Ä–æ–≤–∞|–±—ã–∫|—Ç–µ–ª—è—Ç|—Ç–µ–ª–µ–Ω'
            ]
        },
        'money_wealth': {
            'keywords': ['–¥–µ–Ω—å–≥–∏', '–±–æ–≥–∞—Ç', '–±–µ–¥–µ–Ω', '–∑–æ–ª–æ—Ç–æ', '—Å–µ—Ä–µ–±—Ä–æ', '–∫–æ–ø–µ–π–∫', '—Ä—É–±–ª—å', 
                        '–±–µ–¥–Ω–æ—Å—Ç', '–Ω–∏—â–µ—Ç', '–±–æ–≥–∞—Ç—Å—Ç–≤', '–∫–ª–∞–¥', '—Å–æ–∫—Ä–æ–≤–∏—â', '–º–æ–Ω–µ—Ç', '–≥—Ä–æ—à'],
            'semantic_patterns': [
                r'–¥–µ–Ω—å–≥–∏|–±–æ–≥–∞—Ç|–±–µ–¥–µ–Ω|–Ω–∏—â',
                r'–∑–æ–ª–æ—Ç|—Å–µ—Ä–µ–±—Ä|–∫–æ–ø–µ–π–∫|—Ä—É–±–ª|–≥—Ä–æ—à',
                r'—Å–æ—Å—Ç–æ—è–Ω–∏–µ|–±–æ–≥–∞—Ç—Å—Ç–≤|–±–µ–¥–Ω–æ—Å—Ç'
            ]
        },
        'work_labor': {
            'keywords': ['—Ä–∞–±–æ—Ç', '—Ç—Ä—É–¥', '–¥–µ–ª–æ', '—Å–ª—É–∂–±', '—Ä–µ–º–µ—Å–ª', '–º–∞—Å—Ç–µ—Ä', '–∞—Ä—Ç–µ–ª—å', 
                        '–ø–∞—Ö–∞—Ç—å', '—Å–µ—è—Ç—å', '–∂–∞—Ç—å', '–∫–æ—Å–∏', '–º–æ–ª–æ—Ç', '–∫—É–∑–Ω–µ—Ü', '—Å—Ç–æ–ª—è—Ä', 
                        '–ø–ª–æ—Ç–Ω–∏–∫', '—à–≤–µ—è', '–ø–æ—Ä—Ç–Ω–æ–π'],
            'semantic_patterns': [
                r'—Ä–∞–±–æ—Ç|—Ç—Ä—É–¥|–¥–µ–ª|—Å–ª—É–∂–±',
                r'–º–∞—Å—Ç–µ—Ä|—Ä–µ–º–µ—Å–ª|–∫—É–∑–Ω–µ—Ü|—Å—Ç–æ–ª—è—Ä',
                r'–ø–∞—Ö–∞—Ç—å|—Å–µ—è—Ç—å|–∫–æ—Å–∏—Ç—å|–º–æ–ª–æ—Ç–∏—Ç—å'
            ]
        },
        'time_age': {
            'keywords': ['–≤—Ä–µ–º—è', '–≥–æ–¥', '–¥–µ–Ω—å', '–Ω–æ—á—å', '—É—Ç—Ä–æ', '–≤–µ—á–µ—Ä', '—á–∞—Å', '–º–∏–Ω—É—Ç', 
                        '—Å–µ–∫—É–Ω–¥', '–≤–æ–∑—Ä–∞—Å—Ç', '–º–æ–ª–æ–¥', '—Å—Ç–∞—Ä', '–¥–µ—Ç—Å—Ç–≤', '—é–Ω–æ—Å—Ç—å', '–∑—Ä–µ–ª–æ—Å—Ç', '—Å—Ç–∞—Ä–æ—Å—Ç'],
            'semantic_patterns': [
                r'–≤—Ä–µ–º—è|—á–∞—Å|–º–∏–Ω—É—Ç|—Å–µ–∫—É–Ω–¥',
                r'–¥–µ–Ω—å|–Ω–æ—á—å|—É—Ç—Ä–æ|–≤–µ—á–µ—Ä',
                r'–≥–æ–¥|–º–µ—Å—è—Ü|–Ω–µ–¥–µ–ª—è',
                r'–≤–æ–∑—Ä–∞—Å—Ç|–º–æ–ª–æ–¥|—Å—Ç–∞—Ä|–¥–µ—Ç—Å—Ç–≤|—é–Ω–æ—Å—Ç|—Å—Ç–∞—Ä–æ—Å—Ç'
            ]
        },
        'speech_communication': {
            'keywords': ['–≥–æ–≤–æ—Ä–∏—Ç—å', '—Å–∫–∞–∑–∞—Ç—å', '—Å–ª–æ–≤–æ', '—Ä–µ—á—å', '—è–∑—ã–∫', '–º–æ–ª—á–∞—Ç—å', '–∫—Ä–∏—á–∞—Ç—å', 
                        '—à–µ–ø—Ç–∞—Ç—å', '–±–æ–ª—Ç–∞—Ç—å', '—Ä–∞—Å—Å–∫–∞–∑', '–±–µ—Å–µ–¥–∞', '—Ä–∞–∑–≥–æ–≤–æ—Ä', '—Å–ø–æ—Ä', '—Å—Å–æ—Ä–∞'],
            'semantic_patterns': [
                r'–≥–æ–≤–æ—Ä|—Å–∫–∞–∑–∞—Ç—å|—Å–ª–æ–≤|—Ä–µ—á—å',
                r'–º–æ–ª—á–∞—Ç—å|—à–µ–ø—Ç–∞—Ç—å|–∫—Ä–∏—á–∞—Ç—å|–±–æ–ª—Ç–∞—Ç—å',
                r'—Ä–∞–∑–≥–æ–≤–æ—Ä|–±–µ—Å–µ–¥–∞|—Å–ø–æ—Ä|—Å—Å–æ—Ä–∞'
            ]
        },
        'character_behavior': {
            'keywords': ['—Ö–∞—Ä–∞–∫—Ç–µ—Ä', '–Ω—Ä–∞–≤', '–ø–æ–≤–µ–¥–µ–Ω–∏–µ', '–¥–æ–±—Ä—ã–π', '–∑–ª–æ–π', '—Ö–æ—Ä–æ—à–∏–π', '–ø–ª–æ—Ö–æ–π', 
                        '—á–µ—Å—Ç–Ω—ã–π', '–ª–∂–∏–≤—ã–π', '—Ö—Ä–∞–±—Ä—ã–π', '—Ç—Ä—É—Å–ª–∏–≤—ã–π', '–≥–æ—Ä–¥—ã–π', '—Å–∫—Ä–æ–º–Ω—ã–π', 
                        '–∂–∞–¥–Ω—ã–π', '—â–µ–¥—Ä—ã–π', '–ª–µ–Ω–∏–≤—ã–π', '—Ç—Ä—É–¥–æ–ª—é–±–∏–≤'],
            'semantic_patterns': [
                r'—Ö–∞—Ä–∞–∫—Ç–µ—Ä|–Ω—Ä–∞–≤|–ø–æ–≤–µ–¥–µ–Ω–∏–µ',
                r'–¥–æ–±—Ä|–∑–ª|—Ö–æ—Ä–æ—à|–ø–ª–æ—Ö',
                r'—á–µ—Å—Ç–Ω|–ª–∂–∏–≤|—Ö—Ä–∞–±—Ä|—Ç—Ä—É—Å–ª',
                r'–≥–æ—Ä–¥|—Å–∫—Ä–æ–º–Ω|–∂–∞–¥–Ω|—â–µ–¥—Ä'
            ]
        }
    }
    
    # Analyze each phrase
    for i, phrase_data in enumerate(phrases):
        phrase = phrase_data['phrase'].lower()
        meanings = ' '.join(phrase_data.get('meanings', [])).lower()
        etymology = phrase_data.get('etymology', '').lower()
        current_category = phrase_data['category']
        
        # Combine all text for analysis
        full_text = f"{phrase} {meanings} {etymology}"
        
        # Find the best matching category
        best_category = None
        best_score = 0
        
        for category, rules in categorization_rules.items():
            score = 0
            
            # Check keyword matches
            for keyword in rules['keywords']:
                if keyword in full_text:
                    score += 2
            
            # Check semantic pattern matches
            for pattern in rules['semantic_patterns']:
                if re.search(pattern, full_text):
                    score += 3
            
            if score > best_score:
                best_score = score
                best_category = category
        
        # If we found a better category and current one is wrong
        if best_category and best_category != current_category and best_score >= 2:
            issues.append({
                'index': i,
                'phrase': phrase_data['phrase'],
                'current_category': current_category,
                'suggested_category': best_category,
                'score': best_score,
                'meanings': phrase_data.get('meanings', [])
            })
    
    return issues, categorization_rules

def manual_fixes():
    """Return a list of manual fixes for specific phrases."""
    return {
        # Animals that are incorrectly categorized
        '–±—Ä–∞—Ç—å—è –Ω–∞—à–∏ –º–µ–Ω—å—à–∏–µ': 'animals',
        '–±–∏—Ç—å—Å—è –∫–∞–∫ —Ä—ã–±–∞ –æ–± –ª–µ–¥': 'animals', 
        '–í–∞—Å—å–∫–∞ —Å–ª—É—à–∞–µ—Ç, –¥–∞ –µ—Å—Ç': 'animals',
        '–≤–µ—Ä–Ω—ë–º—Å—è –∫ –Ω–∞—à–∏–º –±–∞—Ä–∞–Ω–∞–º': 'animals',
        '–≤–æ–ª–∫ –≤ –æ–≤–µ—á—å–µ–π —à–∫—É—Ä–µ': 'animals',
        '–≤—Å—Ç–∞–≤–∞—Ç—å —Å –ø–µ—Ç—É—Ö–∞–º–∏': 'animals',
        '–≥–æ–ª–æ–¥–Ω—ã–π –∫–∞–∫ –≤–æ–ª–∫': 'animals',
        '–¥–æ–π–Ω–∞—è –∫–æ—Ä–æ–≤–∞': 'animals',
        '–¥–µ–ª–∞—Ç—å –∏–∑ –º—É—Ö–∏ —Å–ª–æ–Ω–∞': 'animals',
        '–∫–∞–∫ —Å –≥—É—Å—è –≤–æ–¥–∞': 'animals',
        '–∫–æ—Ç –≤ –º–µ—à–∫–µ': 'animals',
        '–∫–æ—Ç –≤–∞–ª—è–∫–∞': 'animals',
        '–∫–æ—Ç –Ω–∞–ø–ª–∞–∫–∞–ª': 'animals',
        '–ª–æ–≤–∏—Ç—å —Ä—ã–±—É –≤ –º—É—Ç–Ω–æ–π –≤–æ–¥–µ': 'animals',
        '–ª–æ–∂–∏—Ç—å—Å—è —Å–ø–∞—Ç—å —Å –∫—É—Ä–∞–º–∏': 'animals',
        '—á—É–≤—Å—Ç–≤–æ–≤–∞—Ç—å —Å–µ–±—è –∫–∞–∫ —Ä—ã–±–∞ –≤ –≤–æ–¥–µ': 'animals',
        '—ë—à–∫–∏–Ω –∫–æ—Ç': 'animals',
        
        # Money/economics related
        '–≥—Ä–æ—à —Ü–µ–Ω–∞': 'money_wealth',
        '–≥—Ä–æ—à–∞ –ª–æ–º–∞–Ω–æ–≥–æ –Ω–µ —Å—Ç–æ–∏—Ç': 'money_wealth',
        
        # Speech/communication
        '–∫—Ä—ã–ª–∞—Ç—ã–µ —Å–ª–æ–≤–∞': 'speech_communication',
        
        # Religion/mythology  
        '—Ç–µ—Ä–Ω–æ–≤—ã–π –≤–µ–Ω–µ—Ü': 'religion_mythology',
        '—Ä–∞–∑—Ä—É–±–∏—Ç—å –≥–æ—Ä–¥–∏–µ–≤ —É–∑–µ–ª': 'religion_mythology',
        
        # Time/weather
        '–∂–¥–∞—Ç—å —É –º–æ—Ä—è –ø–æ–≥–æ–¥—ã': 'weather_nature',
        '–∂–∏–≤, –∫—É—Ä–∏–ª–∫–∞!': 'general',
        
        # Incorrectly categorized as animals
        '–≤ —Ü–µ–Ω—Ç—Ä–µ –≤–Ω–∏–º–∞–Ω–∏—è': 'general',
        '–ª–µ–∑—Ç—å –≤–ø–µ—Ä—ë–¥ –±–∞—Ç—å–∫–∏ –≤ –ø–µ–∫–ª–æ': 'general',
        '–Ω–µ–ø—Ä–µ–∫–ª–æ–Ω–Ω–æ–µ –ø—Ä–∞–≤–∏–ª–æ': 'general',
        '–Ω–µ—á–∏—Å—Ç –Ω–∞ —Ä—É–∫—É': 'character_behavior',
    }

def apply_fixes(data, issues, manual_fix_dict):
    """Apply the categorization fixes to the data."""
    phrases = data['phrases']
    
    # Apply manual fixes first
    manual_fixes_applied = 0
    for phrase_data in phrases:
        phrase = phrase_data['phrase']
        if phrase in manual_fix_dict:
            old_category = phrase_data['category']
            phrase_data['category'] = manual_fix_dict[phrase]
            print(f"Manual fix: '{phrase}' {old_category} ‚Üí {manual_fix_dict[phrase]}")
            manual_fixes_applied += 1
    
    # Apply algorithmic fixes for high-confidence cases
    algorithmic_fixes_applied = 0
    for issue in issues:
        if issue['score'] >= 5:  # High confidence threshold
            phrase_data = phrases[issue['index']]
            old_category = phrase_data['category']
            phrase_data['category'] = issue['suggested_category']
            print(f"Auto fix: '{issue['phrase']}' {old_category} ‚Üí {issue['suggested_category']} (score: {issue['score']})")
            algorithmic_fixes_applied += 1
    
    print(f"\n‚úÖ Applied {manual_fixes_applied} manual fixes")
    print(f"‚úÖ Applied {algorithmic_fixes_applied} algorithmic fixes")
    
    return data

def main():
    """Main function to analyze and fix categorization issues."""
    print("üîß Starting categorization analysis and fixes...")
    
    # Load data
    data = load_phrases()
    
    # Analyze issues
    issues, rules = analyze_categorization_issues(data)
    
    print(f"\nüö® Found {len(issues)} potential categorization issues")
    
    # Show top issues
    print(f"\nüìã Top 20 categorization issues:")
    for issue in sorted(issues, key=lambda x: x['score'], reverse=True)[:20]:
        print(f"  '{issue['phrase']}' - {issue['current_category']} ‚Üí {issue['suggested_category']} (score: {issue['score']})")
        print(f"     Meaning: {issue['meanings'][0] if issue['meanings'] else 'No meaning'}")
    
    # Get manual fixes
    manual_fix_dict = manual_fixes()
    
    # Apply fixes
    fixed_data = apply_fixes(data, issues, manual_fix_dict)
    
    # Save fixed data
    with open('table_phrases_fixed.json', 'w', encoding='utf-8') as f:
        json.dump(fixed_data, f, ensure_ascii=False, indent=2)
    
    print(f"\nüíæ Saved fixed data to table_phrases_fixed.json")
    
    # Final statistics
    category_counts = Counter(phrase['category'] for phrase in fixed_data['phrases'])
    print(f"\nüìä Updated category distribution:")
    for cat, count in category_counts.most_common():
        cat_name = fixed_data['categories'].get(cat, {}).get('name', cat)
        print(f"  {cat_name} ({cat}): {count} phrases")

if __name__ == "__main__":
    main()