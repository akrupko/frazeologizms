#!/usr/bin/env python3
"""
Script to analyze and fix categorization issues in table_phrases.json.
This script identifies phrases that are incorrectly categorized and suggests proper categories.
"""

import json
import re
from collections import defaultdict, Counter

def load_phrases():
    """Load phrases from the JSON file."""
    with open('table_phrases.json', 'r', encoding='utf-8') as f:
        data = json.load(f)
    return data

def analyze_categorization_issues(data):
    """Analyze and identify categorization issues."""
    phrases = data['phrases']
    categories = data['categories']
    
    issues = []
    fixes = []
    
    print("ðŸ” Analyzing categorization issues...")
    print(f"Total phrases: {len(phrases)}")
    
    # Category distribution
    category_counts = Counter(phrase['category'] for phrase in phrases)
    print(f"\nðŸ“Š Current category distribution:")
    for cat, count in category_counts.most_common():
        cat_name = categories.get(cat, {}).get('name', cat)
        print(f"  {cat_name} ({cat}): {count} phrases")
    
    # Define more specific categorization rules based on semantic meaning
    categorization_rules = {
        'emotions_feelings': {
            'keywords': ['Ñ€Ð°Ð´Ð¾ÑÑ‚', 'Ð³Ñ€ÑƒÑÑ‚ÑŒ', 'Ð¿ÐµÑ‡Ð°Ð»ÑŒ', 'ÑÑ‚Ñ€Ð°Ñ…', 'ÑƒÐ¶Ð°Ñ', 'Ð³Ð½ÐµÐ²', 'Ð·Ð»Ð¾ÑÑ‚', 'ÑÑ€Ð¾ÑÑ‚', 
                        'ÑÑ‡Ð°ÑÑ‚ÑŒÐµ', 'Ð³Ð¾Ñ€Ðµ', 'Ñ‚Ð¾ÑÐºÐ°', 'ÑÐºÑƒÐºÐ°', 'Ð²ÐµÑÐµÐ»', 'ÑÐ¼ÐµÑ…', 'ÑÐ»ÐµÐ·Ñ‹', 'Ð¿Ð»Ð°Ñ‡', 
                        'Ð¾Ð±Ð¸Ð´Ð°', 'Ð·Ð°Ð²Ð¸ÑÑ‚ÑŒ', 'Ñ€ÐµÐ²Ð½Ð¾ÑÑ‚', 'Ð»ÑŽÐ±Ð¾Ð²ÑŒ', 'Ð½ÐµÐ½Ð°Ð²Ð¸ÑÑ‚', 'Ð²Ð¾Ð»Ð½ÐµÐ½Ð¸Ðµ', 'Ð±ÐµÑÐ¿Ð¾ÐºÐ¾Ð¹',
                        'Ñ‚Ñ€ÐµÐ²Ð¾Ð³', 'Ð¸ÑÐ¿ÑƒÐ³', 'Ð±Ð¾Ñ', 'Ð¾Ð¿Ð°Ñ'],
            'semantic_patterns': [
                r'Ð±Ð¾ÑÑ‚ÑŒÑÑ|ÑÑ‚Ñ€Ð°ÑˆÐ½Ð¾|Ð¸ÑÐ¿ÑƒÐ³|Ð¿ÑƒÐ³Ð°Ñ‚ÑŒ',
                r'Ñ€Ð°Ð´Ð¾ÑÑ‚ÑŒ|Ð²ÐµÑÐµÐ»ÑŒÐµ|ÑÐ¼ÐµÑ…|ÑƒÐ»Ñ‹Ð±ÐºÐ°',
                r'Ð³Ñ€ÑƒÑÑ‚ÑŒ|Ð¿ÐµÑ‡Ð°Ð»ÑŒ|Ð³Ð¾Ñ€Ðµ|ÑÐ»ÐµÐ·Ñ‹',
                r'Ð³Ð½ÐµÐ²|Ð·Ð»Ð¾ÑÑ‚ÑŒ|ÑÑ€Ð¾ÑÑ‚ÑŒ|Ñ€Ð°Ð·Ð´Ñ€Ð°Ð¶ÐµÐ½Ð¸Ðµ'
            ]
        },
        'body_parts': {
            'keywords': ['Ð³Ð¾Ð»Ð¾Ð²Ð°', 'Ð³Ð»Ð°Ð·', 'ÑƒÑ…Ð¾', 'Ð½Ð¾Ñ', 'Ñ€Ð¾Ñ‚', 'ÑÐ·Ñ‹Ðº', 'Ð·ÑƒÐ±', 'Ñ€ÑƒÐºÐ°', 'Ð½Ð¾Ð³Ð°', 
                        'Ð¿Ð°Ð»ÑŒÑ†', 'Ð¿Ð»ÐµÑ‡Ð¾', 'ÑÐ¿Ð¸Ð½Ð°', 'Ð³Ñ€ÑƒÐ´ÑŒ', 'ÑÐµÑ€Ð´Ñ†Ðµ', 'Ð´ÑƒÑˆÐ°', 'Ð¶Ð¸Ð²Ð¾Ñ‚', 'ÑˆÐµÑ', 
                        'Ð»Ð¸Ñ†Ð¾', 'Ð»Ð¾Ð±', 'Ð±Ð¾Ñ€Ð¾Ð´Ð°', 'Ð²Ð¾Ð»Ð¾Ñ', 'ÐºÐ¾Ð¶Ð°'],
            'semantic_patterns': [
                r'Ð³Ð¾Ð»Ð¾Ð²|Ð³Ð»Ð°Ð·|Ñ€ÑƒÐº|Ð½Ð¾Ð³|ÑÐµÑ€Ð´Ñ†|Ð´ÑƒÑˆ',
                r'ÑÐ·Ñ‹Ðº|Ð·ÑƒÐ±|Ð½Ð¾Ñ|ÑƒÑ…Ð¾|Ð»Ð¸Ñ†Ð¾'
            ]
        },
        'animals': {
            'keywords': ['ÐºÐ¾Ñ‚', 'ÑÐ¾Ð±Ð°Ðº', 'Ð²Ð¾Ð»Ðº', 'Ð»Ð¸ÑÐ°', 'Ð¼ÐµÐ´Ð²ÐµÐ´', 'Ð·Ð°ÑÑ†', 'Ð¼Ñ‹Ñˆ', 'Ð¿Ñ‚Ð¸Ñ†', 'ÐºÐ¾Ð½ÑŒ', 
                        'Ð»Ð¾ÑˆÐ°Ð´', 'ÐºÐ¾Ñ€Ð¾Ð²Ð°', 'Ð¾Ð²Ñ†', 'ÐºÐ¾Ð·ÐµÐ»', 'Ð±Ð°Ñ€Ð°Ð½', 'Ñ€Ñ‹Ð±', 'Ð·Ð¼ÐµÑ', 'Ð»ÐµÐ²', 'Ñ‚Ð¸Ð³Ñ€', 
                        'ÑÐ»Ð¾Ð½', 'Ð¾Ñ€ÐµÐ»', 'Ð²Ð¾Ñ€Ð¾Ð½', 'ÑÐ¾Ñ€Ð¾Ðº', 'Ð²Ð¾Ñ€Ð¾Ð±ÐµÐ¹', 'Ð³ÑƒÑ', 'ÑƒÑ‚Ðº', 'Ð¿ÐµÑ‚ÑƒÑ…', 'ÐºÑƒÑ€Ð¸Ñ†Ð°',
                        'ÑÐ²Ð¸Ð½ÑŒ', 'ÐºÐ¾Ð±Ñ‹Ð»', 'Ð¶ÐµÑ€ÐµÐ±ÐµÑ†', 'Ñ‚ÐµÐ»ÑÑ‚', 'Ð±Ð»Ð¾Ñ…'],
            'semantic_patterns': [
                r'ÐºÐ¾Ñ‚|ÐºÐ¾ÑˆÐº|ÐºÐ¾Ñ‚Ñ‘Ð½Ð¾Ðº|ÐºÐ¾Ñ‚ÐµÐ½Ð¾Ðº',
                r'ÑÐ¾Ð±Ð°Ðº|Ð¿Ñ‘Ñ|Ð¿ÐµÑ|Ñ‰ÐµÐ½Ð¾Ðº',
                r'Ð²Ð¾Ð»Ðº|Ð²Ð¾Ð»Ñ‡',
                r'Ð¼ÐµÐ´Ð²ÐµÐ´|Ð¼Ð¸ÑˆÐº',
                r'Ñ€Ñ‹Ð±|ÐºÐ°Ñ€Ð°ÑÑŒ|Ñ‰ÑƒÐº',
                r'Ð¿Ñ‚Ð¸Ñ†|Ð¿ÐµÑ‚ÑƒÑ…|ÐºÑƒÑ€Ð¸Ñ†Ð°|Ð³ÑƒÑ|ÑƒÑ‚Ðº|Ð²Ð¾Ñ€Ð¾Ð±ÐµÐ¹|Ð²Ð¾Ñ€Ð¾Ð½|Ð¾Ñ€ÐµÐ»',
                r'Ð»Ð¾ÑˆÐ°Ð´|ÐºÐ¾Ð½ÑŒ|ÐºÐ¾Ð±Ñ‹Ð»|Ð¶ÐµÑ€ÐµÐ±ÐµÑ†|Ð¼ÐµÑ€Ð¸Ð½',
                r'ÐºÐ¾Ñ€Ð¾Ð²Ð°|Ð±Ñ‹Ðº|Ñ‚ÐµÐ»ÑÑ‚|Ñ‚ÐµÐ»ÐµÐ½'
            ]
        },
        'money_wealth': {
            'keywords': ['Ð´ÐµÐ½ÑŒÐ³Ð¸', 'Ð±Ð¾Ð³Ð°Ñ‚', 'Ð±ÐµÐ´ÐµÐ½', 'Ð·Ð¾Ð»Ð¾Ñ‚Ð¾', 'ÑÐµÑ€ÐµÐ±Ñ€Ð¾', 'ÐºÐ¾Ð¿ÐµÐ¹Ðº', 'Ñ€ÑƒÐ±Ð»ÑŒ', 
                        'Ð±ÐµÐ´Ð½Ð¾ÑÑ‚', 'Ð½Ð¸Ñ‰ÐµÑ‚', 'Ð±Ð¾Ð³Ð°Ñ‚ÑÑ‚Ð²', 'ÐºÐ»Ð°Ð´', 'ÑÐ¾ÐºÑ€Ð¾Ð²Ð¸Ñ‰', 'Ð¼Ð¾Ð½ÐµÑ‚', 'Ð³Ñ€Ð¾Ñˆ'],
            'semantic_patterns': [
                r'Ð´ÐµÐ½ÑŒÐ³Ð¸|Ð±Ð¾Ð³Ð°Ñ‚|Ð±ÐµÐ´ÐµÐ½|Ð½Ð¸Ñ‰',
                r'Ð·Ð¾Ð»Ð¾Ñ‚|ÑÐµÑ€ÐµÐ±Ñ€|ÐºÐ¾Ð¿ÐµÐ¹Ðº|Ñ€ÑƒÐ±Ð»|Ð³Ñ€Ð¾Ñˆ',
                r'ÑÐ¾ÑÑ‚Ð¾ÑÐ½Ð¸Ðµ|Ð±Ð¾Ð³Ð°Ñ‚ÑÑ‚Ð²|Ð±ÐµÐ´Ð½Ð¾ÑÑ‚'
            ]
        },
        'work_labor': {
            'keywords': ['Ñ€Ð°Ð±Ð¾Ñ‚', 'Ñ‚Ñ€ÑƒÐ´', 'Ð´ÐµÐ»Ð¾', 'ÑÐ»ÑƒÐ¶Ð±', 'Ñ€ÐµÐ¼ÐµÑÐ»', 'Ð¼Ð°ÑÑ‚ÐµÑ€', 'Ð°Ñ€Ñ‚ÐµÐ»ÑŒ', 
                        'Ð¿Ð°Ñ…Ð°Ñ‚ÑŒ', 'ÑÐµÑÑ‚ÑŒ', 'Ð¶Ð°Ñ‚ÑŒ', 'ÐºÐ¾ÑÐ¸', 'Ð¼Ð¾Ð»Ð¾Ñ‚', 'ÐºÑƒÐ·Ð½ÐµÑ†', 'ÑÑ‚Ð¾Ð»ÑÑ€', 
                        'Ð¿Ð»Ð¾Ñ‚Ð½Ð¸Ðº', 'ÑˆÐ²ÐµÑ', 'Ð¿Ð¾Ñ€Ñ‚Ð½Ð¾Ð¹'],
            'semantic_patterns': [
                r'Ñ€Ð°Ð±Ð¾Ñ‚|Ñ‚Ñ€ÑƒÐ´|Ð´ÐµÐ»|ÑÐ»ÑƒÐ¶Ð±',
                r'Ð¼Ð°ÑÑ‚ÐµÑ€|Ñ€ÐµÐ¼ÐµÑÐ»|ÐºÑƒÐ·Ð½ÐµÑ†|ÑÑ‚Ð¾Ð»ÑÑ€',
                r'Ð¿Ð°Ñ…Ð°Ñ‚ÑŒ|ÑÐµÑÑ‚ÑŒ|ÐºÐ¾ÑÐ¸Ñ‚ÑŒ|Ð¼Ð¾Ð»Ð¾Ñ‚Ð¸Ñ‚ÑŒ'
            ]
        },
        'time_age': {
            'keywords': ['Ð²Ñ€ÐµÐ¼Ñ', 'Ð³Ð¾Ð´', 'Ð´ÐµÐ½ÑŒ', 'Ð½Ð¾Ñ‡ÑŒ', 'ÑƒÑ‚Ñ€Ð¾', 'Ð²ÐµÑ‡ÐµÑ€', 'Ñ‡Ð°Ñ', 'Ð¼Ð¸Ð½ÑƒÑ‚', 
                        'ÑÐµÐºÑƒÐ½Ð´', 'Ð²Ð¾Ð·Ñ€Ð°ÑÑ‚', 'Ð¼Ð¾Ð»Ð¾Ð´', 'ÑÑ‚Ð°Ñ€', 'Ð´ÐµÑ‚ÑÑ‚Ð²', 'ÑŽÐ½Ð¾ÑÑ‚ÑŒ', 'Ð·Ñ€ÐµÐ»Ð¾ÑÑ‚', 'ÑÑ‚Ð°Ñ€Ð¾ÑÑ‚'],
            'semantic_patterns': [
                r'Ð²Ñ€ÐµÐ¼Ñ|Ñ‡Ð°Ñ|Ð¼Ð¸Ð½ÑƒÑ‚|ÑÐµÐºÑƒÐ½Ð´',
                r'Ð´ÐµÐ½ÑŒ|Ð½Ð¾Ñ‡ÑŒ|ÑƒÑ‚Ñ€Ð¾|Ð²ÐµÑ‡ÐµÑ€',
                r'Ð³Ð¾Ð´|Ð¼ÐµÑÑÑ†|Ð½ÐµÐ´ÐµÐ»Ñ',
                r'Ð²Ð¾Ð·Ñ€Ð°ÑÑ‚|Ð¼Ð¾Ð»Ð¾Ð´|ÑÑ‚Ð°Ñ€|Ð´ÐµÑ‚ÑÑ‚Ð²|ÑŽÐ½Ð¾ÑÑ‚|ÑÑ‚Ð°Ñ€Ð¾ÑÑ‚'
            ]
        },
        'speech_communication': {
            'keywords': ['Ð³Ð¾Ð²Ð¾Ñ€Ð¸Ñ‚ÑŒ', 'ÑÐºÐ°Ð·Ð°Ñ‚ÑŒ', 'ÑÐ»Ð¾Ð²Ð¾', 'Ñ€ÐµÑ‡ÑŒ', 'ÑÐ·Ñ‹Ðº', 'Ð¼Ð¾Ð»Ñ‡Ð°Ñ‚ÑŒ', 'ÐºÑ€Ð¸Ñ‡Ð°Ñ‚ÑŒ', 
                        'ÑˆÐµÐ¿Ñ‚Ð°Ñ‚ÑŒ', 'Ð±Ð¾Ð»Ñ‚Ð°Ñ‚ÑŒ', 'Ñ€Ð°ÑÑÐºÐ°Ð·', 'Ð±ÐµÑÐµÐ´Ð°', 'Ñ€Ð°Ð·Ð³Ð¾Ð²Ð¾Ñ€', 'ÑÐ¿Ð¾Ñ€', 'ÑÑÐ¾Ñ€Ð°'],
            'semantic_patterns': [
                r'Ð³Ð¾Ð²Ð¾Ñ€|ÑÐºÐ°Ð·Ð°Ñ‚ÑŒ|ÑÐ»Ð¾Ð²|Ñ€ÐµÑ‡ÑŒ',
                r'Ð¼Ð¾Ð»Ñ‡Ð°Ñ‚ÑŒ|ÑˆÐµÐ¿Ñ‚Ð°Ñ‚ÑŒ|ÐºÑ€Ð¸Ñ‡Ð°Ñ‚ÑŒ|Ð±Ð¾Ð»Ñ‚Ð°Ñ‚ÑŒ',
                r'Ñ€Ð°Ð·Ð³Ð¾Ð²Ð¾Ñ€|Ð±ÐµÑÐµÐ´Ð°|ÑÐ¿Ð¾Ñ€|ÑÑÐ¾Ñ€Ð°'
            ]
        },
        'character_behavior': {
            'keywords': ['Ñ…Ð°Ñ€Ð°ÐºÑ‚ÐµÑ€', 'Ð½Ñ€Ð°Ð²', 'Ð¿Ð¾Ð²ÐµÐ´ÐµÐ½Ð¸Ðµ', 'Ð´Ð¾Ð±Ñ€Ñ‹Ð¹', 'Ð·Ð»Ð¾Ð¹', 'Ñ…Ð¾Ñ€Ð¾ÑˆÐ¸Ð¹', 'Ð¿Ð»Ð¾Ñ…Ð¾Ð¹', 
                        'Ñ‡ÐµÑÑ‚Ð½Ñ‹Ð¹', 'Ð»Ð¶Ð¸Ð²Ñ‹Ð¹', 'Ñ…Ñ€Ð°Ð±Ñ€Ñ‹Ð¹', 'Ñ‚Ñ€ÑƒÑÐ»Ð¸Ð²Ñ‹Ð¹', 'Ð³Ð¾Ñ€Ð´Ñ‹Ð¹', 'ÑÐºÑ€Ð¾Ð¼Ð½Ñ‹Ð¹', 
                        'Ð¶Ð°Ð´Ð½Ñ‹Ð¹', 'Ñ‰ÐµÐ´Ñ€Ñ‹Ð¹', 'Ð»ÐµÐ½Ð¸Ð²Ñ‹Ð¹', 'Ñ‚Ñ€ÑƒÐ´Ð¾Ð»ÑŽÐ±Ð¸Ð²'],
            'semantic_patterns': [
                r'Ñ…Ð°Ñ€Ð°ÐºÑ‚ÐµÑ€|Ð½Ñ€Ð°Ð²|Ð¿Ð¾Ð²ÐµÐ´ÐµÐ½Ð¸Ðµ',
                r'Ð´Ð¾Ð±Ñ€|Ð·Ð»|Ñ…Ð¾Ñ€Ð¾Ñˆ|Ð¿Ð»Ð¾Ñ…',
                r'Ñ‡ÐµÑÑ‚Ð½|Ð»Ð¶Ð¸Ð²|Ñ…Ñ€Ð°Ð±Ñ€|Ñ‚Ñ€ÑƒÑÐ»',
                r'Ð³Ð¾Ñ€Ð´|ÑÐºÑ€Ð¾Ð¼Ð½|Ð¶Ð°Ð´Ð½|Ñ‰ÐµÐ´Ñ€'
            ]
        }
    }
    
    # Analyze each phrase
    for i, phrase_data in enumerate(phrases):
        phrase = phrase_data['phrase'].lower()
        meanings = ' '.join(phrase_data.get('meanings', [])).lower()
        etymology = phrase_data.get('etymology', '').lower()
        current_category = phrase_data['category']
        
        # Combine all text for analysis
        full_text = f"{phrase} {meanings} {etymology}"
        
        # Find the best matching category
        best_category = None
        best_score = 0
        
        for category, rules in categorization_rules.items():
            score = 0
            
            # Check keyword matches
            for keyword in rules['keywords']:
                if keyword in full_text:
                    score += 2
            
            # Check semantic pattern matches
            for pattern in rules['semantic_patterns']:
                if re.search(pattern, full_text):
                    score += 3
            
            if score > best_score:
                best_score = score
                best_category = category
        
        # If we found a better category and current one is wrong
        if best_category and best_category != current_category and best_score >= 2:
            issues.append({
                'index': i,
                'phrase': phrase_data['phrase'],
                'current_category': current_category,
                'suggested_category': best_category,
                'score': best_score,
                'meanings': phrase_data.get('meanings', [])
            })
    
    return issues, categorization_rules

def manual_fixes():
    """Return a list of manual fixes for specific phrases."""
    return {
        # Animals that are incorrectly categorized
        'Ð±Ñ€Ð°Ñ‚ÑŒÑ Ð½Ð°ÑˆÐ¸ Ð¼ÐµÐ½ÑŒÑˆÐ¸Ðµ': 'animals',
        'Ð±Ð¸Ñ‚ÑŒÑÑ ÐºÐ°Ðº Ñ€Ñ‹Ð±Ð° Ð¾Ð± Ð»ÐµÐ´': 'animals', 
        'Ð’Ð°ÑÑŒÐºÐ° ÑÐ»ÑƒÑˆÐ°ÐµÑ‚, Ð´Ð° ÐµÑÑ‚': 'animals',
        'Ð²ÐµÑ€Ð½Ñ‘Ð¼ÑÑ Ðº Ð½Ð°ÑˆÐ¸Ð¼ Ð±Ð°Ñ€Ð°Ð½Ð°Ð¼': 'animals',
        'Ð²Ð¾Ð»Ðº Ð² Ð¾Ð²ÐµÑ‡ÑŒÐµÐ¹ ÑˆÐºÑƒÑ€Ðµ': 'animals',
        'Ð²ÑÑ‚Ð°Ð²Ð°Ñ‚ÑŒ Ñ Ð¿ÐµÑ‚ÑƒÑ…Ð°Ð¼Ð¸': 'animals',
        'Ð³Ð¾Ð»Ð¾Ð´Ð½Ñ‹Ð¹ ÐºÐ°Ðº Ð²Ð¾Ð»Ðº': 'animals',
        'Ð´Ð¾Ð¹Ð½Ð°Ñ ÐºÐ¾Ñ€Ð¾Ð²Ð°': 'animals',
        'Ð´ÐµÐ»Ð°Ñ‚ÑŒ Ð¸Ð· Ð¼ÑƒÑ…Ð¸ ÑÐ»Ð¾Ð½Ð°': 'animals',
        'ÐºÐ°Ðº Ñ Ð³ÑƒÑÑ Ð²Ð¾Ð´Ð°': 'animals',
        'ÐºÐ¾Ñ‚ Ð² Ð¼ÐµÑˆÐºÐµ': 'animals',
        'ÐºÐ¾Ñ‚ Ð²Ð°Ð»ÑÐºÐ°': 'animals',
        'ÐºÐ¾Ñ‚ Ð½Ð°Ð¿Ð»Ð°ÐºÐ°Ð»': 'animals',
        'Ð»Ð¾Ð²Ð¸Ñ‚ÑŒ Ñ€Ñ‹Ð±Ñƒ Ð² Ð¼ÑƒÑ‚Ð½Ð¾Ð¹ Ð²Ð¾Ð´Ðµ': 'animals',
        'Ð»Ð¾Ð¶Ð¸Ñ‚ÑŒÑÑ ÑÐ¿Ð°Ñ‚ÑŒ Ñ ÐºÑƒÑ€Ð°Ð¼Ð¸': 'animals',
        'Ñ‡ÑƒÐ²ÑÑ‚Ð²Ð¾Ð²Ð°Ñ‚ÑŒ ÑÐµÐ±Ñ ÐºÐ°Ðº Ñ€Ñ‹Ð±Ð° Ð² Ð²Ð¾Ð´Ðµ': 'animals',
        'Ñ‘ÑˆÐºÐ¸Ð½ ÐºÐ¾Ñ‚': 'animals',
        
        # Money/economics related
        'Ð³Ñ€Ð¾Ñˆ Ñ†ÐµÐ½Ð°': 'money_wealth',
        'Ð³Ñ€Ð¾ÑˆÐ° Ð»Ð¾Ð¼Ð°Ð½Ð¾Ð³Ð¾ Ð½Ðµ ÑÑ‚Ð¾Ð¸Ñ‚': 'money_wealth',
        
        # Speech/communication
        'ÐºÑ€Ñ‹Ð»Ð°Ñ‚Ñ‹Ðµ ÑÐ»Ð¾Ð²Ð°': 'speech_communication',
        
        # Religion/mythology  
        'Ñ‚ÐµÑ€Ð½Ð¾Ð²Ñ‹Ð¹ Ð²ÐµÐ½ÐµÑ†': 'religion_mythology',
        'Ñ€Ð°Ð·Ñ€ÑƒÐ±Ð¸Ñ‚ÑŒ Ð³Ð¾Ñ€Ð´Ð¸ÐµÐ² ÑƒÐ·ÐµÐ»': 'religion_mythology',
        
        # Time/weather
        'Ð¶Ð´Ð°Ñ‚ÑŒ Ñƒ Ð¼Ð¾Ñ€Ñ Ð¿Ð¾Ð³Ð¾Ð´Ñ‹': 'weather_nature',
        'Ð¶Ð¸Ð², ÐºÑƒÑ€Ð¸Ð»ÐºÐ°!': 'general',
        
        # Incorrectly categorized as animals
        'Ð² Ñ†ÐµÐ½Ñ‚Ñ€Ðµ Ð²Ð½Ð¸Ð¼Ð°Ð½Ð¸Ñ': 'general',
        'Ð»ÐµÐ·Ñ‚ÑŒ Ð²Ð¿ÐµÑ€Ñ‘Ð´ Ð±Ð°Ñ‚ÑŒÐºÐ¸ Ð² Ð¿ÐµÐºÐ»Ð¾': 'general',
        'Ð½ÐµÐ¿Ñ€ÐµÐºÐ»Ð¾Ð½Ð½Ð¾Ðµ Ð¿Ñ€Ð°Ð²Ð¸Ð»Ð¾': 'general',
        'Ð½ÐµÑ‡Ð¸ÑÑ‚ Ð½Ð° Ñ€ÑƒÐºÑƒ': 'character_behavior',
    }

def apply_fixes(data, issues, manual_fix_dict):
    """Apply the categorization fixes to the data."""
    phrases = data['phrases']
    
    # Apply manual fixes first
    manual_fixes_applied = 0
    for phrase_data in phrases:
        phrase = phrase_data['phrase']
        if phrase in manual_fix_dict:
            old_category = phrase_data['category']
            phrase_data['category'] = manual_fix_dict[phrase]
            print(f"Manual fix: '{phrase}' {old_category} â†’ {manual_fix_dict[phrase]}")
            manual_fixes_applied += 1
    
    # Apply algorithmic fixes for high-confidence cases
    algorithmic_fixes_applied = 0
    for issue in issues:
        if issue['score'] >= 5:  # High confidence threshold
            phrase_data = phrases[issue['index']]
            old_category = phrase_data['category']
            phrase_data['category'] = issue['suggested_category']
            print(f"Auto fix: '{issue['phrase']}' {old_category} â†’ {issue['suggested_category']} (score: {issue['score']})")
            algorithmic_fixes_applied += 1
    
    print(f"\nâœ… Applied {manual_fixes_applied} manual fixes")
    print(f"âœ… Applied {algorithmic_fixes_applied} algorithmic fixes")
    
    return data

def main():
    """Main function to analyze and fix categorization issues."""
    print("ðŸ”§ Starting categorization analysis and fixes...")
    
    # Load data
    data = load_phrases()
    
    # Analyze issues
    issues, rules = analyze_categorization_issues(data)
    
    print(f"\nðŸš¨ Found {len(issues)} potential categorization issues")
    
    # Show top issues
    print(f"\nðŸ“‹ Top 20 categorization issues:")
    for issue in sorted(issues, key=lambda x: x['score'], reverse=True)[:20]:
        print(f"  '{issue['phrase']}' - {issue['current_category']} â†’ {issue['suggested_category']} (score: {issue['score']})")
        print(f"     Meaning: {issue['meanings'][0] if issue['meanings'] else 'No meaning'}")
    
    # Get manual fixes
    manual_fix_dict = manual_fixes()
    
    # Apply fixes
    fixed_data = apply_fixes(data, issues, manual_fix_dict)
    
    # Save fixed data
    with open('table_phrases_fixed.json', 'w', encoding='utf-8') as f:
        json.dump(fixed_data, f, ensure_ascii=False, indent=2)
    
    print(f"\nðŸ’¾ Saved fixed data to table_phrases_fixed.json")
    
    # Final statistics
    category_counts = Counter(phrase['category'] for phrase in fixed_data['phrases'])
    print(f"\nðŸ“Š Updated category distribution:")
    for cat, count in category_counts.most_common():
        cat_name = fixed_data['categories'].get(cat, {}).get('name', cat)
        print(f"  {cat_name} ({cat}): {count} phrases")

if __name__ == "__main__":
    main()