#!/usr/bin/env python3
"""
Comprehensive Semantic Categorization Analyzer and Fixer
This script analyzes all phraseological units and corrects categorization based on semantic meaning.
Follows semantic-first categorization principles - the complete expression's meaning determines its category.
"""

import json
import re
from collections import defaultdict, Counter

def load_phrases():
    """Load phrases from the JSON file."""
    with open('table_phrases.json', 'r', encoding='utf-8') as f:
        data = json.load(f)
    return data

def analyze_comprehensive_semantic_categorization(data):
    """Comprehensive analysis and correction based on semantic meaning."""
    phrases = data['phrases']
    categories = data['categories']
    
    corrections = []
    
    print("ðŸ” Starting comprehensive semantic categorization analysis...")
    print(f"Total phrases: {len(phrases)}")
    
    # Enhanced semantic categorization rules based on complete expression meanings
    semantic_rules = {
        'emotions_feelings': {
            'meaning_patterns': [
                r'Ñ€Ð°Ð´Ð¾ÑÑ‚|ÑÑ‡Ð°ÑÑ‚Ð»Ð¸Ð²|Ð²ÐµÑÐµÐ»|ÑÐ¼ÐµÑ…|ÑƒÐ»Ñ‹Ð±Ðº|Ð´Ð¾Ð²Ð¾Ð»ÑŒÐ½',
                r'Ð³Ñ€ÑƒÑÑ‚ÑŒ|Ð¿ÐµÑ‡Ð°Ð»ÑŒ|Ð³Ð¾Ñ€Ðµ|ÑÐ»ÐµÐ·|Ð¿Ð»Ð°Ñ‡|Ñ‚Ð¾ÑÐºÐ°|ÑƒÐ½Ñ‹Ð½',
                r'ÑÑ‚Ñ€Ð°Ñ…|Ð±Ð¾ÑÐ·Ð½|Ð¸ÑÐ¿ÑƒÐ³|ÑƒÐ¶Ð°Ñ|Ñ‚Ñ€ÑƒÑ|Ð¿ÑƒÐ³Ð°Ñ‚',
                r'Ð³Ð½ÐµÐ²|Ð·Ð»Ð¾ÑÑ‚|ÑÑ€Ð¾ÑÑ‚|ÑÐµÑ€Ð´Ð¸Ñ‚|Ñ€Ð°Ð·Ð´Ñ€Ð°Ð¶ÐµÐ½',
                r'Ð»ÑŽÐ±Ð¾Ð²ÑŒ|Ð²Ð»ÑŽÐ±Ð»|ÑÑ‚Ñ€Ð°ÑÑ‚|Ð½ÐµÐ¶Ð½Ð¾ÑÑ‚',
                r'Ð½ÐµÐ½Ð°Ð²Ð¸ÑÑ‚|Ð²Ñ€Ð°Ð¶Ð´ÐµÐ±|Ð¿Ñ€ÐµÐ·Ñ€ÐµÐ½',
                r'Ð²Ð¾Ð»Ð½ÐµÐ½Ð¸Ðµ|Ð±ÐµÑÐ¿Ð¾ÐºÐ¾Ð¹|Ñ‚Ñ€ÐµÐ²Ð¾Ð³|Ð½ÐµÑ€Ð²Ð½',
                r'ÑÑ‚Ñ‹Ð´|ÑÑ€Ð°Ð¼|Ð¿Ð¾Ð·Ð¾Ñ€|ÑÐ¼ÑƒÑ‰ÐµÐ½',
                r'Ð·Ð°Ð²Ð¸ÑÑ‚ÑŒ|Ñ€ÐµÐ²Ð½Ð¾ÑÑ‚',
                r'ÑƒÐ´Ð¸Ð²Ð»ÐµÐ½Ð¸Ðµ|Ð¸Ð·ÑƒÐ¼Ð»ÐµÐ½|Ð¿Ð¾Ñ€Ð°Ð¶ÐµÐ½',
                r'ÑÐ¼Ð¾Ñ†Ð¸Ð¾Ð½Ð°Ð»ÑŒÐ½|Ñ‡ÑƒÐ²ÑÑ‚Ð²|Ð½Ð°ÑÑ‚Ñ€Ð¾ÐµÐ½Ð¸Ðµ|Ð¿ÐµÑ€ÐµÐ¶Ð¸Ð²Ð°Ð½'
            ],
            'description_indicators': [
                'ÑÐ¼Ð¾Ñ†Ð¸Ð¾Ð½Ð°Ð»ÑŒÐ½Ð¾Ðµ ÑÐ¾ÑÑ‚Ð¾ÑÐ½Ð¸Ðµ', 'Ñ‡ÑƒÐ²ÑÑ‚Ð²Ð¾', 'Ð¿ÐµÑ€ÐµÐ¶Ð¸Ð²Ð°Ð½Ð¸Ðµ', 'Ð½Ð°ÑÑ‚Ñ€Ð¾ÐµÐ½Ð¸Ðµ',
                'Ð´ÑƒÑˆÐµÐ²Ð½Ð¾Ðµ Ð²Ð¾Ð»Ð½ÐµÐ½Ð¸Ðµ', 'ÑÐ¼Ð¾Ñ†Ð¸Ñ', 'ÑÐ¸Ð»ÑŒÐ½Ð¾ Ð¿ÐµÑ€ÐµÐ¶Ð¸Ð²Ð°Ñ‚ÑŒ', 'Ð¸ÑÐ¿Ñ‹Ñ‚Ñ‹Ð²Ð°Ñ‚ÑŒ Ñ‡ÑƒÐ²ÑÑ‚Ð²Ð¾'
            ]
        },
        
        'money_wealth': {
            'meaning_patterns': [
                r'Ð±Ð¾Ð³Ð°Ñ‚|ÑÐ¾ÑÑ‚Ð¾ÑÑ‚ÐµÐ»ÑŒÐ½|Ð·Ð°Ð¶Ð¸Ñ‚Ð¾Ñ‡Ð½',
                r'Ð±ÐµÐ´ÐµÐ½|Ð½Ð¸Ñ‰|Ð±ÐµÐ´Ð½Ð¾ÑÑ‚|Ð½ÑƒÐ¶Ð´Ð°',
                r'Ð´ÐµÐ½ÑŒÐ³Ð¸|ÐºÐ°Ð¿Ð¸Ñ‚Ð°Ð»|ÑÑ€ÐµÐ´ÑÑ‚Ð²Ð°|Ñ„Ð¸Ð½Ð°Ð½Ñ',
                r'Ð·Ð¾Ð»Ð¾Ñ‚.*Ð±Ð¾Ð³Ð°Ñ‚|Ð±Ð¾Ð³Ð°Ñ‚.*Ð·Ð¾Ð»Ð¾Ñ‚',
                r'Ð³Ñ€Ð¾Ñˆ|ÐºÐ¾Ð¿ÐµÐ¹Ðº|Ñ€ÑƒÐ±Ð»|Ð¼Ð¾Ð½ÐµÑ‚|Ð²Ð°Ð»ÑŽÑ‚',
                r'ÐºÐ»Ð°Ð´|ÑÐ¾ÐºÑ€Ð¾Ð²Ð¸Ñ‰|Ð±Ð¾Ð³Ð°Ñ‚ÑÑ‚Ð²',
                r'Ð´Ð¾Ð»Ð³|Ð·Ð°ÐµÐ¼|ÐºÑ€ÐµÐ´Ð¸Ñ‚|Ð·Ð°Ð¹Ð¼',
                r'Ð´Ð¾Ñ€Ð¾Ð³|Ð´ÐµÑˆÐµÐ²|Ñ†ÐµÐ½Ð°|ÑÑ‚Ð¾Ð¸Ð¼Ð¾ÑÑ‚',
                r'ÑÐºÐ¾Ð½Ð¾Ð¼Ð¸|Ñ‚Ñ€Ð°Ñ‚Ð¸|Ñ€Ð°ÑÑ…Ð¾Ð´|Ð´Ð¾Ñ…Ð¾Ð´'
            ],
            'description_indicators': [
                'Ð±Ð¾Ð³Ð°Ñ‚ÑÑ‚Ð²Ð¾', 'Ð±ÐµÐ´Ð½Ð¾ÑÑ‚ÑŒ', 'Ð´ÐµÐ½ÑŒÐ³Ð¸', 'Ð¼Ð°Ñ‚ÐµÑ€Ð¸Ð°Ð»ÑŒÐ½Ð¾Ðµ Ð¿Ð¾Ð»Ð¾Ð¶ÐµÐ½Ð¸Ðµ',
                'Ð½Ð¸Ñ‰ÐµÑ‚Ð°', 'Ð´Ð¾ÑÑ‚Ð°Ñ‚Ð¾Ðº', 'ÑÐ¾ÑÑ‚Ð¾ÑÐ½Ð¸Ðµ', 'Ñ„Ð¸Ð½Ð°Ð½ÑÑ‹', 'Ð¼Ð°Ñ‚ÐµÑ€Ð¸Ð°Ð»ÑŒÐ½Ñ‹Ð¹',
                'Ð¾ Ð±Ð¾Ð³Ð°Ñ‚Ñ‹Ñ…', 'Ð¾ Ð±ÐµÐ´Ð½Ñ‹Ñ…', 'Ð¼Ð°Ñ‚ÐµÑ€Ð¸Ð°Ð»ÑŒÐ½Ñ‹Ðµ Ð±Ð»Ð°Ð³Ð°'
            ]
        },
        
        'work_labor': {
            'meaning_patterns': [
                r'Ñ€Ð°Ð±Ð¾Ñ‚|Ñ‚Ñ€ÑƒÐ´|Ñ‚Ñ€ÑƒÐ´Ð¸Ñ‚|Ð´ÐµÑÑ‚ÐµÐ»ÑŒÐ½Ð¾ÑÑ‚',
                r'ÑÐ»ÑƒÐ¶Ð±|ÑÐ»ÑƒÐ¶Ð¸Ñ‚ÑŒ|Ð´Ð¾Ð»Ð¶Ð½Ð¾ÑÑ‚',
                r'Ñ€ÐµÐ¼ÐµÑÐ»|Ð¼Ð°ÑÑ‚ÐµÑ€|Ð¿Ñ€Ð¾Ñ„ÐµÑÑÐ¸Ñ',
                r'Ð¿Ð°Ñ…Ð°Ñ‚ÑŒ|ÑÐµÑÑ‚ÑŒ|ÐºÐ¾ÑÐ¸Ñ‚ÑŒ|Ð¶Ð°Ñ‚ÑŒ',
                r'Ð±ÐµÐ·Ð´ÐµÐ»ÑŒÐ½|Ð»ÐµÐ½Ñ‚ÑÐ¹|Ð»ÐµÐ½Ð¸Ñ‚ÑŒÑÑ|Ð±ÐµÐ·Ð´ÐµÐ»ÑŒÐµ',
                r'ÑƒÑÐ¸Ð»Ð¸|ÑÑ‚Ð°Ñ€Ð°Ñ‚ÑŒÑÑ|Ð½Ð°Ð¿Ñ€ÑÐ¶ÐµÐ½',
                r'Ñ€ÐµÐ·ÑƒÐ»ÑŒÑ‚Ð°Ñ‚|Ð´Ð¾ÑÑ‚Ð¸Ð¶ÐµÐ½|ÑƒÑÐ¿ÐµÑ….*Ð´ÐµÐ»',
                r'Ð²Ñ‹Ð¿Ð¾Ð»Ð½Ñ|Ð¸ÑÐ¿Ð¾Ð»Ð½Ñ|Ð·Ð°Ð²ÐµÑ€ÑˆÐ°Ñ‚ÑŒ'
            ],
            'description_indicators': [
                'Ñ€Ð°Ð±Ð¾Ñ‚Ð°', 'Ñ‚Ñ€ÑƒÐ´', 'Ð´ÐµÑÑ‚ÐµÐ»ÑŒÐ½Ð¾ÑÑ‚ÑŒ', 'Ð¿Ñ€Ð¾Ñ„ÐµÑÑÐ¸Ñ', 'Ñ€ÐµÐ¼ÐµÑÐ»Ð¾',
                'ÑÐ»ÑƒÐ¶Ð±Ð°', 'Ð´ÐµÐ»Ð¾', 'Ð·Ð°Ð½ÑÑ‚Ð¸Ðµ', 'ÑƒÑÐ¸Ð»Ð¸Ñ', 'ÑÑ‚Ð°Ñ€Ð°Ð½Ð¸Ðµ',
                'Ð¾ Ñ€Ð°Ð±Ð¾Ñ‚Ðµ', 'Ñ‚Ñ€ÑƒÐ´Ð¾Ð²Ð°Ñ Ð´ÐµÑÑ‚ÐµÐ»ÑŒÐ½Ð¾ÑÑ‚ÑŒ', 'Ð¿Ñ€Ð¾Ñ„ÐµÑÑÐ¸Ð¾Ð½Ð°Ð»ÑŒÐ½Ð°Ñ Ð´ÐµÑÑ‚ÐµÐ»ÑŒÐ½Ð¾ÑÑ‚ÑŒ'
            ]
        },
        
        'character_behavior': {
            'meaning_patterns': [
                r'Ñ…Ð°Ñ€Ð°ÐºÑ‚ÐµÑ€|Ð½Ñ€Ð°Ð²|Ð½Ð°Ñ‚ÑƒÑ€|Ñ‚ÐµÐ¼Ð¿ÐµÑ€Ð°Ð¼ÐµÐ½Ñ‚',
                r'Ð´Ð¾Ð±Ñ€|Ð·Ð»|Ñ…Ð¾Ñ€Ð¾Ñˆ|Ð¿Ð»Ð¾Ñ…|Ð¼Ð¸Ð»Ð¾ÑÐµÑ€Ð´',
                r'Ñ‡ÐµÑÑ‚Ð½|Ð»Ð¶Ð¸Ð²|Ð¾Ð±Ð¼Ð°Ð½|Ð¿Ñ€Ð°Ð²Ð´Ð¸Ð²',
                r'Ñ…Ñ€Ð°Ð±Ñ€|Ñ‚Ñ€ÑƒÑÐ»|ÑÐ¼ÐµÐ»|Ð¾Ñ‚Ð²Ð°Ð¶Ð½',
                r'Ð³Ð¾Ñ€Ð´|ÑÐºÑ€Ð¾Ð¼Ð½|Ñ…Ð²Ð°ÑÑ‚Ð»Ð¸Ð²|ÑÐ°Ð¼Ð¾Ð»ÑŽÐ±',
                r'Ð¶Ð°Ð´Ð½|Ñ‰ÐµÐ´Ñ€|ÑÐºÑƒÐ¿Ð¾Ð¹|Ñ€Ð°ÑÑ‚Ð¾Ñ‡Ð¸Ñ‚ÐµÐ»ÑŒÐ½',
                r'Ð»ÐµÐ½Ð¸Ð²Ð¾|Ñ‚Ñ€ÑƒÐ´Ð¾Ð»ÑŽÐ±Ð¸Ð²|Ð°ÐºÑ‚Ð¸Ð²Ð½',
                r'Ð¿Ð¾Ð²ÐµÐ´ÐµÐ½Ð¸Ðµ|Ð¿Ð¾ÑÑ‚ÑƒÐ¿Ð¾Ðº|Ð¼Ð°Ð½ÐµÑ€|Ð¿Ñ€Ð¸Ð²Ñ‹Ñ‡Ðº'
            ],
            'description_indicators': [
                'Ñ…Ð°Ñ€Ð°ÐºÑ‚ÐµÑ€', 'Ð¿Ð¾Ð²ÐµÐ´ÐµÐ½Ð¸Ðµ', 'Ð½Ñ€Ð°Ð²', 'ÐºÐ°Ñ‡ÐµÑÑ‚Ð²Ð¾ Ð»Ð¸Ñ‡Ð½Ð¾ÑÑ‚Ð¸',
                'Ñ‡ÐµÑ€Ñ‚Ð° Ñ…Ð°Ñ€Ð°ÐºÑ‚ÐµÑ€Ð°', 'Ð¿Ð¾ÑÑ‚ÑƒÐ¿Ð¾Ðº', 'Ð¼Ð°Ð½ÐµÑ€Ð° Ð¿Ð¾Ð²ÐµÐ´ÐµÐ½Ð¸Ñ',
                'Ð¾ Ñ‡ÐµÐ»Ð¾Ð²ÐµÐºÐµ', 'Ð»Ð¸Ñ‡Ð½Ð¾ÑÑ‚Ð½Ñ‹Ðµ ÐºÐ°Ñ‡ÐµÑÑ‚Ð²Ð°', 'Ð¼Ð¾Ñ€Ð°Ð»ÑŒÐ½Ñ‹Ðµ ÐºÐ°Ñ‡ÐµÑÑ‚Ð²Ð°'
            ]
        },
        
        'speech_communication': {
            'meaning_patterns': [
                r'Ð³Ð¾Ð²Ð¾Ñ€|ÑÐºÐ°Ð·Ð°Ñ‚ÑŒ|Ñ€ÐµÑ‡ÑŒ|Ð±ÐµÑÐµÐ´Ð°',
                r'ÑÐ»Ð¾Ð²|ÑÐ·Ñ‹Ðº|Ð±Ð¾Ð»Ñ‚Ð°Ñ‚ÑŒ|Ñ€Ð°Ð·Ð³Ð¾Ð²Ð°Ñ€',
                r'Ð¼Ð¾Ð»Ñ‡Ð°Ñ‚ÑŒ|Ð±ÐµÐ·Ð¼Ð¾Ð»Ð²|Ñ‚Ð¸ÑˆÐ¸Ð½Ð°',
                r'ÐºÑ€Ð¸Ñ‡Ð°Ñ‚ÑŒ|ÑˆÐµÐ¿Ñ‚Ð°Ñ‚ÑŒ|Ð¾Ñ€Ð°Ñ‚ÑŒ',
                r'ÑÐ¿Ð¾Ñ€|ÑÑÐ¾Ñ€Ð°|Ñ€ÑƒÐ³Ð°Ñ‚ÑŒ|Ð±Ñ€Ð°Ð½Ð¸Ñ‚ÑŒ',
                r'Ñ…Ð²Ð°Ð»Ð¸Ñ‚ÑŒ|Ð¾Ð´Ð¾Ð±Ñ€ÑÑ‚ÑŒ|ÐºÑ€Ð¸Ñ‚Ð¸ÐºÐ¾Ð²',
                r'Ð¾Ð±Ñ‰ÐµÐ½Ð¸Ðµ|ÐºÐ¾Ð¼Ð¼ÑƒÐ½Ð¸ÐºÐ°Ñ†|Ð´Ð¸Ð°Ð»Ð¾Ð³'
            ],
            'description_indicators': [
                'Ñ€ÐµÑ‡ÑŒ', 'Ð¾Ð±Ñ‰ÐµÐ½Ð¸Ðµ', 'Ñ€Ð°Ð·Ð³Ð¾Ð²Ð¾Ñ€', 'ÑÐ»Ð¾Ð²Ð°', 'ÑÐ·Ñ‹Ðº',
                'Ð±ÐµÑÐµÐ´Ð°', 'Ð¼Ð¾Ð»Ñ‡Ð°Ð½Ð¸Ðµ', 'Ð³Ð¾Ð²Ð¾Ñ€Ð¸Ñ‚ÑŒ', 'ÑÐºÐ°Ð·Ð°Ñ‚ÑŒ',
                'Ð¾ Ñ€ÐµÑ‡Ð¸', 'Ð¾ ÑÐ»Ð¾Ð²Ð°Ñ…', 'Ð¾ Ñ€Ð°Ð·Ð³Ð¾Ð²Ð¾Ñ€Ðµ'
            ]
        },
        
        'time_age': {
            'meaning_patterns': [
                r'Ð²Ñ€ÐµÐ¼Ñ|Ñ‡Ð°Ñ|Ð¼Ð¸Ð½ÑƒÑ‚|ÑÐµÐºÑƒÐ½Ð´',
                r'Ð²Ð¾Ð·Ñ€Ð°ÑÑ‚|Ð»ÐµÑ‚|Ð³Ð¾Ð´|ÑÑ‚Ð¾Ð»ÐµÑ‚',
                r'Ð¼Ð¾Ð»Ð¾Ð´|ÑÑ‚Ð°Ñ€|Ð´ÐµÑ‚ÑÑ‚Ð²|ÑŽÐ½Ð¾ÑÑ‚|Ð·Ñ€ÐµÐ»Ð¾ÑÑ‚',
                r'Ð´ÐµÐ½ÑŒ|Ð½Ð¾Ñ‡ÑŒ|ÑƒÑ‚Ñ€Ð¾|Ð²ÐµÑ‡ÐµÑ€',
                r'ÑÐµÐ·Ð¾Ð½|Ð·Ð¸Ð¼Ð°|Ð»ÐµÑ‚Ð¾|Ð²ÐµÑÐ½Ð°|Ð¾ÑÐµÐ½ÑŒ',
                r'Ð¿ÐµÑ€Ð¸Ð¾Ð´|ÑÐ¿Ð¾Ñ…Ð°|Ð²ÐµÐºÐ°|Ñ‚Ñ‹ÑÑÑ‡ÐµÐ»ÐµÑ‚',
                r'Ñ€Ð°Ð½Ð¾|Ð¿Ð¾Ð·Ð´Ð½Ð¾|ÑÐ²Ð¾ÐµÐ²Ñ€ÐµÐ¼ÐµÐ½Ð½|Ð½ÐµÑÐ²Ð¾ÐµÐ²Ñ€ÐµÐ¼ÐµÐ½Ð½',
                r'Ð´Ð¾Ð»Ð³Ð¾|Ð±Ñ‹ÑÑ‚Ñ€Ð¾|Ð¼ÐµÐ´Ð»ÐµÐ½Ð½Ð¾.*Ð²Ñ€ÐµÐ¼ÐµÐ½'
            ],
            'description_indicators': [
                'Ð²Ñ€ÐµÐ¼Ñ', 'Ð²Ð¾Ð·Ñ€Ð°ÑÑ‚', 'Ð¿ÐµÑ€Ð¸Ð¾Ð´', 'ÑÐ¿Ð¾Ñ…Ð°', 'Ð¼Ð¾Ð»Ð¾Ð´Ð¾ÑÑ‚ÑŒ',
                'ÑÑ‚Ð°Ñ€Ð¾ÑÑ‚ÑŒ', 'Ð´ÐµÑ‚ÑÑ‚Ð²Ð¾', 'Ð¾ Ð²Ñ€ÐµÐ¼ÐµÐ½Ð¸', 'Ð²Ñ€ÐµÐ¼ÐµÐ½Ð½Ð¾Ð¹',
                'Ð²Ð¾Ð·Ñ€Ð°ÑÑ‚Ð½Ð¾Ð¹', 'Ð¶Ð¸Ð·Ð½ÐµÐ½Ð½Ñ‹Ð¹ Ð¿ÐµÑ€Ð¸Ð¾Ð´'
            ]
        },
        
        'mind_intelligence': {
            'meaning_patterns': [
                r'ÑƒÐ¼|ÑƒÐ¼Ð½Ñ‹Ð¹|ÑƒÐ¼ÐµÐ½|Ð¼ÑƒÐ´Ñ€',
                r'Ð³Ð»ÑƒÐ¿|Ð´ÑƒÑ€Ð°Ðº|Ð´ÑƒÑ€Ð°|Ð³Ð»ÑƒÐ¿Ð¾ÑÑ‚|Ñ‚ÑƒÐ¿Ð¾Ð¹',
                r'Ñ€Ð°Ð·ÑƒÐ¼|Ñ€Ð°ÑÑÑƒÐ´Ð¾Ðº|Ð¸Ð½Ñ‚ÐµÐ»Ð»ÐµÐºÑ‚',
                r'Ð¿Ð°Ð¼ÑÑ‚ÑŒ|Ð¿Ð¾Ð¼Ð½Ð¸Ñ‚ÑŒ|Ð·Ð°Ð±Ñ‹Ð²|Ð²ÑÐ¿Ð¾Ð¼Ð¸Ð½Ð°',
                r'Ð´ÑƒÐ¼Ð°Ñ‚ÑŒ|Ð¼Ñ‹ÑÐ»|ÑÐ¾Ð¾Ð±Ñ€Ð°Ð¶|Ð¿Ð¾Ð½Ð¸Ð¼Ð°',
                r'Ð·Ð½Ð°Ñ‚ÑŒ|ÑƒÑ‡Ð¸Ñ‚ÑŒ|Ð¾Ð±Ñ€Ð°Ð·Ð¾Ð²Ð°Ð½|Ð½Ð°ÑƒÐº',
                r'Ð¼Ð¾Ð·Ð³|Ð³Ð¾Ð»Ð¾Ð².*ÑƒÐ¼|ÑÐ¾Ð¾Ð±Ñ€Ð°Ð·Ð¸Ñ‚ÐµÐ»ÑŒÐ½'
            ],
            'description_indicators': [
                'ÑƒÐ¼', 'Ð¸Ð½Ñ‚ÐµÐ»Ð»ÐµÐºÑ‚', 'Ñ€Ð°Ð·ÑƒÐ¼', 'Ð³Ð»ÑƒÐ¿Ð¾ÑÑ‚ÑŒ', 'Ð¼Ñ‹ÑˆÐ»ÐµÐ½Ð¸Ðµ',
                'Ð¿Ð°Ð¼ÑÑ‚ÑŒ', 'Ð·Ð½Ð°Ð½Ð¸Ðµ', 'Ð¾ Ð³Ð»ÑƒÐ¿Ð¾Ð¼', 'Ð¾Ð± ÑƒÐ¼Ð½Ð¾Ð¼',
                'Ð¸Ð½Ñ‚ÐµÐ»Ð»ÐµÐºÑ‚ÑƒÐ°Ð»ÑŒÐ½Ñ‹Ðµ ÑÐ¿Ð¾ÑÐ¾Ð±Ð½Ð¾ÑÑ‚Ð¸', 'ÑƒÐ¼ÑÑ‚Ð²ÐµÐ½Ð½Ñ‹Ðµ ÑÐ¿Ð¾ÑÐ¾Ð±Ð½Ð¾ÑÑ‚Ð¸'
            ]
        },
        
        'food_drink': {
            'meaning_patterns': [
                r'ÐµÑÑ‚ÑŒ|Ð¿Ð¸Ñ‚ÑŒ|ÐµÐ´Ð°|Ð¿Ð¸Ñ‚ÑŒÐµ|ÐºÑƒÑˆÐ°Ñ‚ÑŒ',
                r'Ñ…Ð»ÐµÐ±|Ð¼ÑÑÐ¾|Ð¼Ð¾Ð»Ð¾ÐºÐ¾|ÐºÐ°ÑˆÐ°|ÑÑƒÐ¿',
                r'Ð²Ð¾Ð´Ð°|Ð²Ð¸Ð½Ð¾|Ð¿Ð¸Ð²Ð¾|Ñ‡Ð°Ð¹|ÐºÐ¾Ñ„Ðµ',
                r'Ð³Ð¾Ð»Ð¾Ð´|ÑÑ‹Ñ‚|Ð°Ð¿Ð¿ÐµÑ‚Ð¸Ñ‚|Ð²ÐºÑƒÑ',
                r'Ð¾Ð±ÐµÐ´|ÑƒÐ¶Ð¸Ð½|Ð·Ð°Ð²Ñ‚Ñ€Ð°Ðº|Ñ‚Ñ€Ð°Ð¿ÐµÐ·',
                r'ÐºÑƒÑ…Ð½Ñ|ÑÑ‚Ð¾Ð».*ÐµÐ´Ð°|Ð±Ð»ÑŽÐ´Ð¾',
                r'Ð½Ð°Ð¿Ð¸Ñ‚Ð¾Ðº|ÑƒÐ³Ð¾Ñ‰ÐµÐ½|Ð·Ð°ÑÑ‚Ð¾Ð»ÑŒÐµ'
            ],
            'description_indicators': [
                'ÐµÐ´Ð°', 'Ð¿Ð¸Ñ‚ÑŒÐµ', 'Ð¿Ð¸Ñ‰Ð°', 'Ð½Ð°Ð¿Ð¸Ñ‚ÐºÐ¸', 'Ð¾ ÐµÐ´Ðµ',
                'Ð¾ Ð¿Ð¸Ñ‚ÑŒÐµ', 'ÐºÑƒÐ»Ð¸Ð½Ð°Ñ€Ð¸Ñ', 'Ð·Ð°ÑÑ‚Ð¾Ð»ÑŒÐµ', 'Ð¾ Ñ€ÑŽÐ¼ÐºÐµ',
                'Ð¾ Ð½Ð°Ð¿Ð¸Ñ‚ÐºÐµ', 'Ð¿Ð¸Ñ‰ÐµÐ²Ð¾Ð¹', 'Ð¿Ð¸Ñ‚ÐµÐ¹Ð½Ñ‹Ð¹'
            ]
        },
        
        'body_parts': {
            'meaning_patterns': [
                # Only if actually about physical body parts, not metaphorical
                r'Ñ„Ð¸Ð·Ð¸Ñ‡ÐµÑÐº.*Ñ‚ÐµÐ»|Ð°Ð½Ð°Ñ‚Ð¾Ð¼Ð¸|Ñ‚ÐµÐ»ÐµÑÐ½',
                r'Ð¾Ñ€Ð³Ð°Ð½.*Ñ‚ÐµÐ»|Ñ‡Ð°ÑÑ‚ÑŒ.*Ñ‚ÐµÐ»',
                r'Ð±Ð¾Ð»ÐµÐ·Ð½.*Ñ‚ÐµÐ»|Ð·Ð´Ð¾Ñ€Ð¾Ð²ÑŒÐµ.*Ñ‚ÐµÐ»'
            ],
            'description_indicators': [
                'Ñ„Ð¸Ð·Ð¸Ñ‡ÐµÑÐºÐ¾Ðµ', 'Ð°Ð½Ð°Ñ‚Ð¾Ð¼Ð¸Ñ', 'Ñ‚ÐµÐ»ÐµÑÐ½Ñ‹Ð¹', 'Ð¾Ñ€Ð³Ð°Ð½',
                'Ñ‡Ð°ÑÑ‚ÑŒ Ñ‚ÐµÐ»Ð°', 'Ð¾ Ñ‚ÐµÐ»Ðµ', 'Ñ„Ð¸Ð·Ð¸Ð¾Ð»Ð¾Ð³Ð¸Ñ'
            ],
            # Special handling: if body part mentioned but meaning is metaphorical, don't categorize here
            'avoid_if_metaphorical': True
        },
        
        'animals': {
            'meaning_patterns': [
                # Only if actually about animals as living creatures
                r'Ð¶Ð¸Ð²Ð¾Ñ‚Ð½|Ð·Ð²ÐµÑ€ÑŒ|ÑÐºÐ¾Ñ‚|Ñ„Ð°ÑƒÐ½Ð°',
                r'Ð¿Ð¾Ð²ÐµÐ´ÐµÐ½Ð¸Ðµ.*Ð¶Ð¸Ð²Ð¾Ñ‚Ð½|Ð¿Ð¾Ð²Ð°Ð´ÐºÐ¸.*Ð¶Ð¸Ð²Ð¾Ñ‚Ð½',
                r'Ð¾Ñ…Ð¾Ñ‚Ð°|Ð·Ð²ÐµÑ€Ð¾Ð»Ð¾|Ð¶Ð¸Ð²Ð¾Ñ‚Ð½Ð¾Ð²Ð¾Ð´'
            ],
            'description_indicators': [
                'Ð¶Ð¸Ð²Ð¾Ñ‚Ð½Ñ‹Ðµ', 'Ð·Ð²ÐµÑ€Ð¸', 'Ð¾ Ð¶Ð¸Ð²Ð¾Ñ‚Ð½Ñ‹Ñ…', 'Ð·Ð¾Ð¾Ð»Ð¾Ð³Ð¸Ñ',
                'Ð¶Ð¸Ð²Ð¾Ñ‚Ð½Ñ‹Ð¹ Ð¼Ð¸Ñ€', 'Ñ„Ð°ÑƒÐ½Ð°'
            ],
            # Special handling: if animal mentioned but meaning is metaphorical, don't categorize here
            'avoid_if_metaphorical': True
        },
        
        'quantity_measure': {
            'meaning_patterns': [
                r'Ð¼Ð½Ð¾Ð³Ð¾|Ð¼Ð°Ð»Ð¾|ÐºÐ¾Ð»Ð¸Ñ‡ÐµÑÑ‚Ð²|Ñ‡Ð¸ÑÐ»',
                r'Ð±Ð¾Ð»ÑŒÑˆ|Ð¼Ð°Ð»ÐµÐ½ÑŒÐº|Ð¾Ð³Ñ€Ð¾Ð¼Ð½|ÐºÑ€Ð¾ÑˆÐµÑ‡Ð½',
                r'Ñ€Ð°Ð·Ð¼ÐµÑ€|Ð¼ÐµÑ€Ð°|Ð²ÐµÐ»Ð¸Ñ‡Ð¸Ð½|Ð¾Ð±ÑŠÐµÐ¼',
                r'Ð´Ð»Ð¸Ð½Ð½|ÐºÐ¾Ñ€Ð¾Ñ‚Ðº|Ð²Ñ‹ÑÐ¾Ðº|Ð½Ð¸Ð·Ðº',
                r'ÑˆÐ¸Ñ€Ð¾Ðº|ÑƒÐ·Ðº|Ñ‚Ð¾Ð»ÑÑ‚|Ñ‚Ð¾Ð½Ðº',
                r'Ð²ÐµÑ|Ð»ÐµÐ³Ðº|Ñ‚ÑÐ¶ÐµÐ»'
            ],
            'description_indicators': [
                'ÐºÐ¾Ð»Ð¸Ñ‡ÐµÑÑ‚Ð²Ð¾', 'Ñ€Ð°Ð·Ð¼ÐµÑ€', 'Ð¼ÐµÑ€Ð°', 'Ð¼Ð½Ð¾Ð³Ð¾', 'Ð¼Ð°Ð»Ð¾',
                'Ð±Ð¾Ð»ÑŒÑˆÐ¾Ð¹', 'Ð¼Ð°Ð»ÐµÐ½ÑŒÐºÐ¸Ð¹', 'Ð¾ ÐºÐ¾Ð»Ð¸Ñ‡ÐµÑÑ‚Ð²Ðµ', 'Ð¾ Ñ€Ð°Ð·Ð¼ÐµÑ€Ðµ'
            ]
        },
        
        'general': {
            'description_indicators': [
                'Ð¾Ð±Ñ‰ÐµÐµ Ð²Ñ‹Ñ€Ð°Ð¶ÐµÐ½Ð¸Ðµ', 'Ñ€Ð°Ð·Ð½Ð¾Ðµ', 'Ñ€Ð°Ð·Ð»Ð¸Ñ‡Ð½Ð¾Ðµ'
            ],
            # General category for expressions that don't fit specific categories
            'is_fallback': True
        }
    }
    
    # Analyze each phrase
    for i, phrase_data in enumerate(phrases):
        phrase = phrase_data['phrase'].lower()
        meanings = ' '.join(phrase_data.get('meanings', [])).lower()
        etymology = phrase_data.get('etymology', '').lower()
        current_category = phrase_data['category']
        
        # Combine all text for analysis
        full_text = f"{phrase} {meanings} {etymology}"
        
        # Find the best matching category based on semantic meaning
        best_category = None
        best_score = 0
        category_scores = {}
        
        for category, rules in semantic_rules.items():
            score = 0
            
            # Skip fallback categories in initial scoring
            if rules.get('is_fallback'):
                continue
            
            # Check description indicators (highest weight) - these are in the meaning text
            if 'description_indicators' in rules:
                for indicator in rules['description_indicators']:
                    if indicator in meanings:
                        score += 10  # High weight for semantic meaning indicators
            
            # Check meaning patterns in the meanings text (high weight)
            if 'meaning_patterns' in rules:
                for pattern in rules['meaning_patterns']:
                    if re.search(pattern, meanings):
                        score += 5  # Medium-high weight for meaning patterns
            
            # Special handling for body_parts and animals - avoid if metaphorical
            if category in ['body_parts', 'animals'] and rules.get('avoid_if_metaphorical'):
                # Check if it's actually about the physical aspect
                physical_indicators = [
                    'Ñ„Ð¸Ð·Ð¸Ñ‡ÐµÑÐºÐ¸', 'Ð°Ð½Ð°Ñ‚Ð¾Ð¼Ð¸Ñ', 'Ð±ÑƒÐºÐ²Ð°Ð»ÑŒÐ½Ð¾', 'Ð½Ð°ÑÑ‚Ð¾ÑÑ‰Ð¸Ð¹',
                    'Ñ€ÐµÐ°Ð»ÑŒÐ½Ñ‹Ð¹', 'Ð¶Ð¸Ð²Ð¾Ðµ', 'Ð±Ð¸Ð¾Ð»Ð¾Ð³Ð¸Ñ'
                ]
                is_physical = any(indicator in full_text for indicator in physical_indicators)
                
                # If no physical indicators but score > 0, it's likely metaphorical
                if score > 0 and not is_physical:
                    # Check if there are strong semantic indicators for other categories
                    has_other_semantic = False
                    for other_cat, other_rules in semantic_rules.items():
                        if other_cat != category and 'description_indicators' in other_rules:
                            for other_indicator in other_rules['description_indicators']:
                                if other_indicator in meanings:
                                    has_other_semantic = True
                                    break
                    
                    if has_other_semantic:
                        score = max(1, score - 8)  # Reduce score significantly for metaphorical usage
            
            category_scores[category] = score
            
            if score > best_score:
                best_score = score
                best_category = category
        
        # Special cases and validation
        
        # 1. Time-related phrases should stay in time_age if they're about time duration
        if current_category == 'time_age':
            time_indicators = ['Ð²Ñ€ÐµÐ¼Ñ', 'Ð¿ÐµÑ€Ð¸Ð¾Ð´', 'Ð´Ð¾Ð»Ð³Ð¾', 'Ð±Ñ‹ÑÑ‚Ñ€Ð¾', 'Ñ€Ð°Ð½Ð¾', 'Ð¿Ð¾Ð·Ð´Ð½Ð¾', 'Ð²Ñ€ÐµÐ¼ÐµÐ½Ð½Ð¾Ð¹']
            if any(indicator in meanings for indicator in time_indicators):
                # Keep in time_age if it's genuinely about time
                continue
        
        # 2. Work-related phrases about effort vs results
        effort_result_patterns = [
            r'ÑƒÑÐ¸Ð»Ð¸.*Ñ€ÐµÐ·ÑƒÐ»ÑŒÑ‚Ð°Ñ‚|Ñ€ÐµÐ·ÑƒÐ»ÑŒÑ‚Ð°Ñ‚.*ÑƒÑÐ¸Ð»Ð¸',
            r'Ñ‚Ñ€ÑƒÐ´.*Ð¿Ð¾Ð»ÑŒÐ·|Ð¿Ð¾Ð»ÑŒÐ·.*Ñ‚Ñ€ÑƒÐ´',
            r'Ñ€Ð°Ð±Ð¾Ñ‚.*Ð²Ñ‹Ð³Ð¾Ð´|Ð²Ñ‹Ð³Ð¾Ð´.*Ñ€Ð°Ð±Ð¾Ñ‚',
            r'ÑÑ‚Ð°Ñ€Ð°Ñ‚ÑŒÑÑ.*Ñ‚Ð¾Ð»Ðº|Ñ‚Ð¾Ð»Ðº.*ÑÑ‚Ð°Ñ€Ð°Ñ‚ÑŒÑÑ',
            r'Ð´ÐµÐ»Ð¾.*ÑÑ‚Ð¾Ð¸Ñ‚|ÑÑ‚Ð¾Ð¸Ñ‚.*Ð´ÐµÐ»Ð¾'
        ]
        for pattern in effort_result_patterns:
            if re.search(pattern, meanings):
                if best_category != 'work_labor':
                    best_category = 'work_labor'
                    best_score = 8
                    break
        
        # 3. Money/wealth related expressions
        wealth_patterns = [
            r'Ð±Ð¾Ð³Ð°Ñ‚.*Ð´ÐµÑ‚ÐµÐ¹|Ð´ÐµÑ‚ÐµÐ¹.*Ð±Ð¾Ð³Ð°Ñ‚',
            r'Ð¼Ð°Ñ‚ÐµÑ€Ð¸Ð°Ð»ÑŒÐ½.*Ð¿Ð¾Ð»Ð¾Ð¶ÐµÐ½|Ð¿Ð¾Ð»Ð¾Ð¶ÐµÐ½.*Ð¼Ð°Ñ‚ÐµÑ€Ð¸Ð°Ð»ÑŒÐ½',
            r'Ð´ÐµÐ½ÑŒÐ³Ð¸.*Ñ‚Ñ€Ð°Ñ‚Ð¸Ñ‚ÑŒ|Ñ‚Ñ€Ð°Ñ‚Ð¸Ñ‚ÑŒ.*Ð´ÐµÐ½ÑŒÐ³Ð¸'
        ]
        for pattern in wealth_patterns:
            if re.search(pattern, meanings):
                if best_category != 'money_wealth':
                    best_category = 'money_wealth'
                    best_score = 8
                    break
        
        # If we found a significantly better category
        if best_category and best_category != current_category and best_score >= 5:
            corrections.append({
                'index': i,
                'phrase': phrase_data['phrase'],
                'current_category': current_category,
                'suggested_category': best_category,
                'score': best_score,
                'meaning': phrase_data.get('meanings', [''])[0] if phrase_data.get('meanings') else '',
                'reason': f'Semantic analysis (score: {best_score})',
                'category_scores': category_scores
            })
    
    return corrections

def apply_semantic_corrections(data, corrections, apply_threshold=5):
    """Apply semantic corrections to the data."""
    phrases = data['phrases']
    
    applied_count = 0
    skipped_count = 0
    
    print(f"\nðŸ”„ Applying corrections with confidence score >= {apply_threshold}...")
    
    for correction in corrections:
        if correction['score'] >= apply_threshold:
            phrase_data = phrases[correction['index']]
            old_category = phrase_data['category']
            phrase_data['category'] = correction['suggested_category']
            
            print(f"âœ… '{correction['phrase'][:50]}{'...' if len(correction['phrase']) > 50 else ''}' ")
            print(f"   {old_category} â†’ {correction['suggested_category']} (score: {correction['score']})")
            print(f"   Meaning: {correction['meaning'][:80]}{'...' if len(correction['meaning']) > 80 else ''}")
            print()
            
            applied_count += 1
        else:
            skipped_count += 1
    
    print(f"ðŸ“Š Applied {applied_count} corrections, skipped {skipped_count} low-confidence suggestions")
    return data

def main():
    """Main function to analyze and fix comprehensive semantic categorization."""
    print("ðŸ”§ Starting comprehensive semantic categorization analysis...")
    
    # Load data
    data = load_phrases()
    
    # Analyze semantic issues
    corrections = analyze_comprehensive_semantic_categorization(data)
    
    print(f"\nðŸš¨ Found {len(corrections)} potential semantic corrections")
    
    # Show statistics by category
    current_categories = Counter([c['current_category'] for c in corrections])
    suggested_categories = Counter([c['suggested_category'] for c in corrections])
    
    print(f"\nðŸ“ˆ Categories with most issues:")
    for cat, count in current_categories.most_common(10):
        print(f"  {cat}: {count} phrases need correction")
    
    print(f"\nðŸ“ˆ Most suggested target categories:")
    for cat, count in suggested_categories.most_common(10):
        print(f"  {cat}: {count} phrases should move here")
    
    # Show top corrections by confidence
    print(f"\nðŸ“‹ Top 20 high-confidence corrections:")
    for correction in sorted(corrections, key=lambda x: x['score'], reverse=True)[:20]:
        print(f"  '{correction['phrase'][:40]}{'...' if len(correction['phrase']) > 40 else ''}' ")
        print(f"    {correction['current_category']} â†’ {correction['suggested_category']} (score: {correction['score']})")
        print(f"    {correction['meaning'][:60]}{'...' if len(correction['meaning']) > 60 else ''}")
        print()
    
    # Apply high-confidence corrections
    print(f"\nðŸ”„ Applying high-confidence corrections (score >= 5)...")
    fixed_data = apply_semantic_corrections(data, corrections, apply_threshold=5)
    
    # Save corrected data
    with open('table_phrases.json', 'w', encoding='utf-8') as f:
        json.dump(fixed_data, f, ensure_ascii=False, indent=2)
    
    print(f"\nðŸ’¾ Applied corrections to table_phrases.json")
    
    # Also save a backup with all suggestions for review
    with open('semantic_corrections_report.json', 'w', encoding='utf-8') as f:
        json.dump({
            'applied_corrections': [c for c in corrections if c['score'] >= 5],
            'suggested_corrections': [c for c in corrections if c['score'] < 5],
            'statistics': {
                'total_corrections': len(corrections),
                'applied': len([c for c in corrections if c['score'] >= 5]),
                'suggested': len([c for c in corrections if c['score'] < 5]),
                'categories_with_issues': dict(current_categories),
                'target_categories': dict(suggested_categories)
            }
        }, f, ensure_ascii=False, indent=2)
    
    print(f"ðŸ“‹ Detailed report saved to semantic_corrections_report.json")
    
    return corrections

if __name__ == "__main__":
    corrections = main()