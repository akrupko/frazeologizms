#!/usr/bin/env python3
"""
Comprehensive Semantic Categorization Analyzer and Fixer
This script analyzes all phraseological units and corrects categorization based on semantic meaning.
Follows semantic-first categorization principles - the complete expression's meaning determines its category.
"""

import json
import re
from collections import defaultdict, Counter

def load_phrases():
    """Load phrases from the JSON file."""
    with open('table_phrases.json', 'r', encoding='utf-8') as f:
        data = json.load(f)
    return data

def analyze_comprehensive_semantic_categorization(data):
    """Comprehensive analysis and correction based on semantic meaning."""
    phrases = data['phrases']
    categories = data['categories']
    
    corrections = []
    
    print("üîç Starting comprehensive semantic categorization analysis...")
    print(f"Total phrases: {len(phrases)}")
    
    # Enhanced semantic categorization rules based on complete expression meanings
    semantic_rules = {
        'emotions_feelings': {
            'meaning_patterns': [
                r'—Ä–∞–¥–æ—Å—Ç|—Å—á–∞—Å—Ç–ª–∏–≤|–≤–µ—Å–µ–ª|—Å–º–µ—Ö|—É–ª—ã–±–∫|–¥–æ–≤–æ–ª—å–Ω',
                r'–≥—Ä—É—Å—Ç—å|–ø–µ—á–∞–ª—å|–≥–æ—Ä–µ|—Å–ª–µ–∑|–ø–ª–∞—á|—Ç–æ—Å–∫–∞|—É–Ω—ã–Ω',
                r'—Å—Ç—Ä–∞—Ö|–±–æ—è–∑–Ω|–∏—Å–ø—É–≥|—É–∂–∞—Å|—Ç—Ä—É—Å|–ø—É–≥–∞—Ç',
                r'–≥–Ω–µ–≤|–∑–ª–æ—Å—Ç|—è—Ä–æ—Å—Ç|—Å–µ—Ä–¥–∏—Ç|—Ä–∞–∑–¥—Ä–∞–∂–µ–Ω',
                r'–ª—é–±–æ–≤—å|–≤–ª—é–±–ª|—Å—Ç—Ä–∞—Å—Ç|–Ω–µ–∂–Ω–æ—Å—Ç',
                r'–Ω–µ–Ω–∞–≤–∏—Å—Ç|–≤—Ä–∞–∂–¥–µ–±|–ø—Ä–µ–∑—Ä–µ–Ω',
                r'–≤–æ–ª–Ω–µ–Ω–∏–µ|–±–µ—Å–ø–æ–∫–æ–π|—Ç—Ä–µ–≤–æ–≥|–Ω–µ—Ä–≤–Ω',
                r'—Å—Ç—ã–¥|—Å—Ä–∞–º|–ø–æ–∑–æ—Ä|—Å–º—É—â–µ–Ω',
                r'–∑–∞–≤–∏—Å—Ç—å|—Ä–µ–≤–Ω–æ—Å—Ç',
                r'—É–¥–∏–≤–ª–µ–Ω–∏–µ|–∏–∑—É–º–ª–µ–Ω|–ø–æ—Ä–∞–∂–µ–Ω',
                r'—ç–º–æ—Ü–∏–æ–Ω–∞–ª—å–Ω|—á—É–≤—Å—Ç–≤|–Ω–∞—Å—Ç—Ä–æ–µ–Ω–∏–µ|–ø–µ—Ä–µ–∂–∏–≤–∞–Ω'
            ],
            'description_indicators': [
                '—ç–º–æ—Ü–∏–æ–Ω–∞–ª—å–Ω–æ–µ —Å–æ—Å—Ç–æ—è–Ω–∏–µ', '—á—É–≤—Å—Ç–≤–æ', '–ø–µ—Ä–µ–∂–∏–≤–∞–Ω–∏–µ', '–Ω–∞—Å—Ç—Ä–æ–µ–Ω–∏–µ',
                '–¥—É—à–µ–≤–Ω–æ–µ –≤–æ–ª–Ω–µ–Ω–∏–µ', '—ç–º–æ—Ü–∏—è', '—Å–∏–ª—å–Ω–æ –ø–µ—Ä–µ–∂–∏–≤–∞—Ç—å', '–∏—Å–ø—ã—Ç—ã–≤–∞—Ç—å —á—É–≤—Å—Ç–≤–æ'
            ]
        },
        
        'money_wealth': {
            'meaning_patterns': [
                r'–±–æ–≥–∞—Ç|—Å–æ—Å—Ç–æ—è—Ç–µ–ª—å–Ω|–∑–∞–∂–∏—Ç–æ—á–Ω',
                r'–±–µ–¥–µ–Ω|–Ω–∏—â|–±–µ–¥–Ω–æ—Å—Ç|–Ω—É–∂–¥–∞',
                r'–¥–µ–Ω—å–≥–∏|–∫–∞–ø–∏—Ç–∞–ª|—Å—Ä–µ–¥—Å—Ç–≤–∞|—Ñ–∏–Ω–∞–Ω—Å',
                r'–∑–æ–ª–æ—Ç.*–±–æ–≥–∞—Ç|–±–æ–≥–∞—Ç.*–∑–æ–ª–æ—Ç',
                r'–≥—Ä–æ—à|–∫–æ–ø–µ–π–∫|—Ä—É–±–ª|–º–æ–Ω–µ—Ç|–≤–∞–ª—é—Ç',
                r'–∫–ª–∞–¥|—Å–æ–∫—Ä–æ–≤–∏—â|–±–æ–≥–∞—Ç—Å—Ç–≤',
                r'–¥–æ–ª–≥|–∑–∞–µ–º|–∫—Ä–µ–¥–∏—Ç|–∑–∞–π–º',
                r'–¥–æ—Ä–æ–≥|–¥–µ—à–µ–≤|—Ü–µ–Ω–∞|—Å—Ç–æ–∏–º–æ—Å—Ç',
                r'—ç–∫–æ–Ω–æ–º–∏|—Ç—Ä–∞—Ç–∏|—Ä–∞—Å—Ö–æ–¥|–¥–æ—Ö–æ–¥'
            ],
            'description_indicators': [
                '–±–æ–≥–∞—Ç—Å—Ç–≤–æ', '–±–µ–¥–Ω–æ—Å—Ç—å', '–¥–µ–Ω—å–≥–∏', '–º–∞—Ç–µ—Ä–∏–∞–ª—å–Ω–æ–µ –ø–æ–ª–æ–∂–µ–Ω–∏–µ',
                '–Ω–∏—â–µ—Ç–∞', '–¥–æ—Å—Ç–∞—Ç–æ–∫', '—Å–æ—Å—Ç–æ—è–Ω–∏–µ', '—Ñ–∏–Ω–∞–Ω—Å—ã', '–º–∞—Ç–µ—Ä–∏–∞–ª—å–Ω—ã–π',
                '–æ –±–æ–≥–∞—Ç—ã—Ö', '–æ –±–µ–¥–Ω—ã—Ö', '–º–∞—Ç–µ—Ä–∏–∞–ª—å–Ω—ã–µ –±–ª–∞–≥–∞'
            ]
        },
        
        'work_labor': {
            'meaning_patterns': [
                r'—Ä–∞–±–æ—Ç|—Ç—Ä—É–¥|—Ç—Ä—É–¥–∏—Ç|–¥–µ—è—Ç–µ–ª—å–Ω–æ—Å—Ç',
                r'—Å–ª—É–∂–±|—Å–ª—É–∂–∏—Ç—å|–¥–æ–ª–∂–Ω–æ—Å—Ç',
                r'—Ä–µ–º–µ—Å–ª|–º–∞—Å—Ç–µ—Ä|–ø—Ä–æ—Ñ–µ—Å—Å–∏—è',
                r'–ø–∞—Ö–∞—Ç—å|—Å–µ—è—Ç—å|–∫–æ—Å–∏—Ç—å|–∂–∞—Ç—å',
                r'–±–µ–∑–¥–µ–ª—å–Ω|–ª–µ–Ω—Ç—è–π|–ª–µ–Ω–∏—Ç—å—Å—è|–±–µ–∑–¥–µ–ª—å–µ',
                r'—É—Å–∏–ª–∏|—Å—Ç–∞—Ä–∞—Ç—å—Å—è|–Ω–∞–ø—Ä—è–∂–µ–Ω',
                r'—Ä–µ–∑—É–ª—å—Ç–∞—Ç|–¥–æ—Å—Ç–∏–∂–µ–Ω|—É—Å–ø–µ—Ö.*–¥–µ–ª',
                r'–≤—ã–ø–æ–ª–Ω—è|–∏—Å–ø–æ–ª–Ω—è|–∑–∞–≤–µ—Ä—à–∞—Ç—å'
            ],
            'description_indicators': [
                '—Ä–∞–±–æ—Ç–∞', '—Ç—Ä—É–¥', '–¥–µ—è—Ç–µ–ª—å–Ω–æ—Å—Ç—å', '–ø—Ä–æ—Ñ–µ—Å—Å–∏—è', '—Ä–µ–º–µ—Å–ª–æ',
                '—Å–ª—É–∂–±–∞', '–¥–µ–ª–æ', '–∑–∞–Ω—è—Ç–∏–µ', '—É—Å–∏–ª–∏—è', '—Å—Ç–∞—Ä–∞–Ω–∏–µ',
                '–æ —Ä–∞–±–æ—Ç–µ', '—Ç—Ä—É–¥–æ–≤–∞—è –¥–µ—è—Ç–µ–ª—å–Ω–æ—Å—Ç—å', '–ø—Ä–æ—Ñ–µ—Å—Å–∏–æ–Ω–∞–ª—å–Ω–∞—è –¥–µ—è—Ç–µ–ª—å–Ω–æ—Å—Ç—å'
            ]
        },
        
        'character_behavior': {
            'meaning_patterns': [
                r'—Ö–∞—Ä–∞–∫—Ç–µ—Ä|–Ω—Ä–∞–≤|–Ω–∞—Ç—É—Ä|—Ç–µ–º–ø–µ—Ä–∞–º–µ–Ω—Ç',
                r'–¥–æ–±—Ä|–∑–ª|—Ö–æ—Ä–æ—à|–ø–ª–æ—Ö|–º–∏–ª–æ—Å–µ—Ä–¥',
                r'—á–µ—Å—Ç–Ω|–ª–∂–∏–≤|–æ–±–º–∞–Ω|–ø—Ä–∞–≤–¥–∏–≤',
                r'—Ö—Ä–∞–±—Ä|—Ç—Ä—É—Å–ª|—Å–º–µ–ª|–æ—Ç–≤–∞–∂–Ω',
                r'–≥–æ—Ä–¥|—Å–∫—Ä–æ–º–Ω|—Ö–≤–∞—Å—Ç–ª–∏–≤|—Å–∞–º–æ–ª—é–±',
                r'–∂–∞–¥–Ω|—â–µ–¥—Ä|—Å–∫—É–ø–æ–π|—Ä–∞—Å—Ç–æ—á–∏—Ç–µ–ª—å–Ω',
                r'–ª–µ–Ω–∏–≤–æ|—Ç—Ä—É–¥–æ–ª—é–±–∏–≤|–∞–∫—Ç–∏–≤–Ω',
                r'–ø–æ–≤–µ–¥–µ–Ω–∏–µ|–ø–æ—Å—Ç—É–ø–æ–∫|–º–∞–Ω–µ—Ä|–ø—Ä–∏–≤—ã—á–∫'
            ],
            'description_indicators': [
                '—Ö–∞—Ä–∞–∫—Ç–µ—Ä', '–ø–æ–≤–µ–¥–µ–Ω–∏–µ', '–Ω—Ä–∞–≤', '–∫–∞—á–µ—Å—Ç–≤–æ –ª–∏—á–Ω–æ—Å—Ç–∏',
                '—á–µ—Ä—Ç–∞ —Ö–∞—Ä–∞–∫—Ç–µ—Ä–∞', '–ø–æ—Å—Ç—É–ø–æ–∫', '–º–∞–Ω–µ—Ä–∞ –ø–æ–≤–µ–¥–µ–Ω–∏—è',
                '–æ —á–µ–ª–æ–≤–µ–∫–µ', '–ª–∏—á–Ω–æ—Å—Ç–Ω—ã–µ –∫–∞—á–µ—Å—Ç–≤–∞', '–º–æ—Ä–∞–ª—å–Ω—ã–µ –∫–∞—á–µ—Å—Ç–≤–∞'
            ]
        },
        
        'speech_communication': {
            'meaning_patterns': [
                r'–≥–æ–≤–æ—Ä|—Å–∫–∞–∑–∞—Ç—å|—Ä–µ—á—å|–±–µ—Å–µ–¥–∞',
                r'—Å–ª–æ–≤|—è–∑—ã–∫|–±–æ–ª—Ç–∞—Ç—å|—Ä–∞–∑–≥–æ–≤–∞—Ä',
                r'–º–æ–ª—á–∞—Ç—å|–±–µ–∑–º–æ–ª–≤|—Ç–∏—à–∏–Ω–∞',
                r'–∫—Ä–∏—á–∞—Ç—å|—à–µ–ø—Ç–∞—Ç—å|–æ—Ä–∞—Ç—å',
                r'—Å–ø–æ—Ä|—Å—Å–æ—Ä–∞|—Ä—É–≥–∞—Ç—å|–±—Ä–∞–Ω–∏—Ç—å',
                r'—Ö–≤–∞–ª–∏—Ç—å|–æ–¥–æ–±—Ä—è—Ç—å|–∫—Ä–∏—Ç–∏–∫–æ–≤',
                r'–æ–±—â–µ–Ω–∏–µ|–∫–æ–º–º—É–Ω–∏–∫–∞—Ü|–¥–∏–∞–ª–æ–≥'
            ],
            'description_indicators': [
                '—Ä–µ—á—å', '–æ–±—â–µ–Ω–∏–µ', '—Ä–∞–∑–≥–æ–≤–æ—Ä', '—Å–ª–æ–≤–∞', '—è–∑—ã–∫',
                '–±–µ—Å–µ–¥–∞', '–º–æ–ª—á–∞–Ω–∏–µ', '–≥–æ–≤–æ—Ä–∏—Ç—å', '—Å–∫–∞–∑–∞—Ç—å',
                '–æ —Ä–µ—á–∏', '–æ —Å–ª–æ–≤–∞—Ö', '–æ —Ä–∞–∑–≥–æ–≤–æ—Ä–µ'
            ]
        },
        
        'time_age': {
            'meaning_patterns': [
                r'–≤—Ä–µ–º—è|—á–∞—Å|–º–∏–Ω—É—Ç|—Å–µ–∫—É–Ω–¥',
                r'–≤–æ–∑—Ä–∞—Å—Ç|–ª–µ—Ç|–≥–æ–¥|—Å—Ç–æ–ª–µ—Ç',
                r'–º–æ–ª–æ–¥|—Å—Ç–∞—Ä|–¥–µ—Ç—Å—Ç–≤|—é–Ω–æ—Å—Ç|–∑—Ä–µ–ª–æ—Å—Ç',
                r'–¥–µ–Ω—å|–Ω–æ—á—å|—É—Ç—Ä–æ|–≤–µ—á–µ—Ä',
                r'—Å–µ–∑–æ–Ω|–∑–∏–º–∞|–ª–µ—Ç–æ|–≤–µ—Å–Ω–∞|–æ—Å–µ–Ω—å',
                r'–ø–µ—Ä–∏–æ–¥|—ç–ø–æ—Ö–∞|–≤–µ–∫–∞|—Ç—ã—Å—è—á–µ–ª–µ—Ç',
                r'—Ä–∞–Ω–æ|–ø–æ–∑–¥–Ω–æ|—Å–≤–æ–µ–≤—Ä–µ–º–µ–Ω–Ω|–Ω–µ—Å–≤–æ–µ–≤—Ä–µ–º–µ–Ω–Ω',
                r'–¥–æ–ª–≥–æ|–±—ã—Å—Ç—Ä–æ|–º–µ–¥–ª–µ–Ω–Ω–æ.*–≤—Ä–µ–º–µ–Ω'
            ],
            'description_indicators': [
                '–≤—Ä–µ–º—è', '–≤–æ–∑—Ä–∞—Å—Ç', '–ø–µ—Ä–∏–æ–¥', '—ç–ø–æ—Ö–∞', '–º–æ–ª–æ–¥–æ—Å—Ç—å',
                '—Å—Ç–∞—Ä–æ—Å—Ç—å', '–¥–µ—Ç—Å—Ç–≤–æ', '–æ –≤—Ä–µ–º–µ–Ω–∏', '–≤—Ä–µ–º–µ–Ω–Ω–æ–π',
                '–≤–æ–∑—Ä–∞—Å—Ç–Ω–æ–π', '–∂–∏–∑–Ω–µ–Ω–Ω—ã–π –ø–µ—Ä–∏–æ–¥'
            ]
        },
        
        'mind_intelligence': {
            'meaning_patterns': [
                r'—É–º|—É–º–Ω—ã–π|—É–º–µ–Ω|–º—É–¥—Ä',
                r'–≥–ª—É–ø|–¥—É—Ä–∞–∫|–¥—É—Ä–∞|–≥–ª—É–ø–æ—Å—Ç|—Ç—É–ø–æ–π',
                r'—Ä–∞–∑—É–º|—Ä–∞—Å—Å—É–¥–æ–∫|–∏–Ω—Ç–µ–ª–ª–µ–∫—Ç',
                r'–ø–∞–º—è—Ç—å|–ø–æ–º–Ω–∏—Ç—å|–∑–∞–±—ã–≤|–≤—Å–ø–æ–º–∏–Ω–∞',
                r'–¥—É–º–∞—Ç—å|–º—ã—Å–ª|—Å–æ–æ–±—Ä–∞–∂|–ø–æ–Ω–∏–º–∞',
                r'–∑–Ω–∞—Ç—å|—É—á–∏—Ç—å|–æ–±—Ä–∞–∑–æ–≤–∞–Ω|–Ω–∞—É–∫',
                r'–º–æ–∑–≥|–≥–æ–ª–æ–≤.*—É–º|—Å–æ–æ–±—Ä–∞–∑–∏—Ç–µ–ª—å–Ω'
            ],
            'description_indicators': [
                '—É–º', '–∏–Ω—Ç–µ–ª–ª–µ–∫—Ç', '—Ä–∞–∑—É–º', '–≥–ª—É–ø–æ—Å—Ç—å', '–º—ã—à–ª–µ–Ω–∏–µ',
                '–ø–∞–º—è—Ç—å', '–∑–Ω–∞–Ω–∏–µ', '–æ –≥–ª—É–ø–æ–º', '–æ–± —É–º–Ω–æ–º',
                '–∏–Ω—Ç–µ–ª–ª–µ–∫—Ç—É–∞–ª—å–Ω—ã–µ —Å–ø–æ—Å–æ–±–Ω–æ—Å—Ç–∏', '—É–º—Å—Ç–≤–µ–Ω–Ω—ã–µ —Å–ø–æ—Å–æ–±–Ω–æ—Å—Ç–∏'
            ]
        },
        
        'food_drink': {
            'meaning_patterns': [
                r'–µ—Å—Ç—å|–ø–∏—Ç—å|–µ–¥–∞|–ø–∏—Ç—å–µ|–∫—É—à–∞—Ç—å',
                r'—Ö–ª–µ–±|–º—è—Å–æ|–º–æ–ª–æ–∫–æ|–∫–∞—à–∞|—Å—É–ø',
                r'–≤–æ–¥–∞|–≤–∏–Ω–æ|–ø–∏–≤–æ|—á–∞–π|–∫–æ—Ñ–µ',
                r'–≥–æ–ª–æ–¥|—Å—ã—Ç|–∞–ø–ø–µ—Ç–∏—Ç|–≤–∫—É—Å',
                r'–æ–±–µ–¥|—É–∂–∏–Ω|–∑–∞–≤—Ç—Ä–∞–∫|—Ç—Ä–∞–ø–µ–∑',
                r'–∫—É—Ö–Ω—è|—Å—Ç–æ–ª.*–µ–¥–∞|–±–ª—é–¥–æ',
                r'–Ω–∞–ø–∏—Ç–æ–∫|—É–≥–æ—â–µ–Ω|–∑–∞—Å—Ç–æ–ª—å–µ'
            ],
            'description_indicators': [
                '–µ–¥–∞', '–ø–∏—Ç—å–µ', '–ø–∏—â–∞', '–Ω–∞–ø–∏—Ç–∫–∏', '–æ –µ–¥–µ',
                '–æ –ø–∏—Ç—å–µ', '–∫—É–ª–∏–Ω–∞—Ä–∏—è', '–∑–∞—Å—Ç–æ–ª—å–µ', '–æ —Ä—é–º–∫–µ',
                '–æ –Ω–∞–ø–∏—Ç–∫–µ', '–ø–∏—â–µ–≤–æ–π', '–ø–∏—Ç–µ–π–Ω—ã–π'
            ]
        },
        
        'body_parts': {
            'meaning_patterns': [
                # Only if actually about physical body parts, not metaphorical
                r'—Ñ–∏–∑–∏—á–µ—Å–∫.*—Ç–µ–ª|–∞–Ω–∞—Ç–æ–º–∏|—Ç–µ–ª–µ—Å–Ω',
                r'–æ—Ä–≥–∞–Ω.*—Ç–µ–ª|—á–∞—Å—Ç—å.*—Ç–µ–ª',
                r'–±–æ–ª–µ–∑–Ω.*—Ç–µ–ª|–∑–¥–æ—Ä–æ–≤—å–µ.*—Ç–µ–ª'
            ],
            'description_indicators': [
                '—Ñ–∏–∑–∏—á–µ—Å–∫–æ–µ', '–∞–Ω–∞—Ç–æ–º–∏—è', '—Ç–µ–ª–µ—Å–Ω—ã–π', '–æ—Ä–≥–∞–Ω',
                '—á–∞—Å—Ç—å —Ç–µ–ª–∞', '–æ —Ç–µ–ª–µ', '—Ñ–∏–∑–∏–æ–ª–æ–≥–∏—è'
            ],
            # Special handling: if body part mentioned but meaning is metaphorical, don't categorize here
            'avoid_if_metaphorical': True
        },
        
        'animals': {
            'meaning_patterns': [
                # Only if actually about animals as living creatures
                r'–∂–∏–≤–æ—Ç–Ω|–∑–≤–µ—Ä—å|—Å–∫–æ—Ç|—Ñ–∞—É–Ω–∞',
                r'–ø–æ–≤–µ–¥–µ–Ω–∏–µ.*–∂–∏–≤–æ—Ç–Ω|–ø–æ–≤–∞–¥–∫–∏.*–∂–∏–≤–æ—Ç–Ω',
                r'–æ—Ö–æ—Ç–∞|–∑–≤–µ—Ä–æ–ª–æ|–∂–∏–≤–æ—Ç–Ω–æ–≤–æ–¥'
            ],
            'description_indicators': [
                '–∂–∏–≤–æ—Ç–Ω—ã–µ', '–∑–≤–µ—Ä–∏', '–æ –∂–∏–≤–æ—Ç–Ω—ã—Ö', '–∑–æ–æ–ª–æ–≥–∏—è',
                '–∂–∏–≤–æ—Ç–Ω—ã–π –º–∏—Ä', '—Ñ–∞—É–Ω–∞'
            ],
            # Special handling: if animal mentioned but meaning is metaphorical, don't categorize here
            'avoid_if_metaphorical': True
        },
        
        'quantity_measure': {
            'meaning_patterns': [
                r'–º–Ω–æ–≥–æ|–º–∞–ª–æ|–∫–æ–ª–∏—á–µ—Å—Ç–≤|—á–∏—Å–ª',
                r'–±–æ–ª—å—à|–º–∞–ª–µ–Ω—å–∫|–æ–≥—Ä–æ–º–Ω|–∫—Ä–æ—à–µ—á–Ω',
                r'—Ä–∞–∑–º–µ—Ä|–º–µ—Ä–∞|–≤–µ–ª–∏—á–∏–Ω|–æ–±—ä–µ–º',
                r'–¥–ª–∏–Ω–Ω|–∫–æ—Ä–æ—Ç–∫|–≤—ã—Å–æ–∫|–Ω–∏–∑–∫',
                r'—à–∏—Ä–æ–∫|—É–∑–∫|—Ç–æ–ª—Å—Ç|—Ç–æ–Ω–∫',
                r'–≤–µ—Å|–ª–µ–≥–∫|—Ç—è–∂–µ–ª'
            ],
            'description_indicators': [
                '–∫–æ–ª–∏—á–µ—Å—Ç–≤–æ', '—Ä–∞–∑–º–µ—Ä', '–º–µ—Ä–∞', '–º–Ω–æ–≥–æ', '–º–∞–ª–æ',
                '–±–æ–ª—å—à–æ–π', '–º–∞–ª–µ–Ω—å–∫–∏–π', '–æ –∫–æ–ª–∏—á–µ—Å—Ç–≤–µ', '–æ —Ä–∞–∑–º–µ—Ä–µ'
            ]
        },
        
        'general': {
            'description_indicators': [
                '–æ–±—â–µ–µ –≤—ã—Ä–∞–∂–µ–Ω–∏–µ', '—Ä–∞–∑–Ω–æ–µ', '—Ä–∞–∑–ª–∏—á–Ω–æ–µ'
            ],
            # General category for expressions that don't fit specific categories
            'is_fallback': True
        }
    }
    
    # Analyze each phrase
    for i, phrase_data in enumerate(phrases):
        phrase = phrase_data['phrase'].lower()
        meanings = ' '.join(phrase_data.get('meanings', [])).lower()
        etymology = phrase_data.get('etymology', '').lower()
        current_category = phrase_data['category']
        
        # Combine all text for analysis
        full_text = f"{phrase} {meanings} {etymology}"
        
        # Find the best matching category based on semantic meaning
        best_category = None
        best_score = 0
        category_scores = {}
        
        for category, rules in semantic_rules.items():
            score = 0
            
            # Skip fallback categories in initial scoring
            if rules.get('is_fallback'):
                continue
            
            # Check description indicators (highest weight) - these are in the meaning text
            if 'description_indicators' in rules:
                for indicator in rules['description_indicators']:
                    if indicator in meanings:
                        score += 10  # High weight for semantic meaning indicators
            
            # Check meaning patterns in the meanings text (high weight)
            if 'meaning_patterns' in rules:
                for pattern in rules['meaning_patterns']:
                    if re.search(pattern, meanings):
                        score += 5  # Medium-high weight for meaning patterns
            
            # Special handling for body_parts and animals - avoid if metaphorical
            if category in ['body_parts', 'animals'] and rules.get('avoid_if_metaphorical'):
                # Check if it's actually about the physical aspect
                physical_indicators = [
                    '—Ñ–∏–∑–∏—á–µ—Å–∫–∏', '–∞–Ω–∞—Ç–æ–º–∏—è', '–±—É–∫–≤–∞–ª—å–Ω–æ', '–Ω–∞—Å—Ç–æ—è—â–∏–π',
                    '—Ä–µ–∞–ª—å–Ω—ã–π', '–∂–∏–≤–æ–µ', '–±–∏–æ–ª–æ–≥–∏—è'
                ]
                is_physical = any(indicator in full_text for indicator in physical_indicators)
                
                # If no physical indicators but score > 0, it's likely metaphorical
                if score > 0 and not is_physical:
                    # Check if there are strong semantic indicators for other categories
                    has_other_semantic = False
                    for other_cat, other_rules in semantic_rules.items():
                        if other_cat != category and 'description_indicators' in other_rules:
                            for other_indicator in other_rules['description_indicators']:
                                if other_indicator in meanings:
                                    has_other_semantic = True
                                    break
                    
                    if has_other_semantic:
                        score = max(1, score - 8)  # Reduce score significantly for metaphorical usage
            
            category_scores[category] = score
            
            if score > best_score:
                best_score = score
                best_category = category
        
        # Special cases and validation
        
        # 1. Time-related phrases should stay in time_age if they're about time duration
        if current_category == 'time_age':
            time_indicators = ['–≤—Ä–µ–º—è', '–ø–µ—Ä–∏–æ–¥', '–¥–æ–ª–≥–æ', '–±—ã—Å—Ç—Ä–æ', '—Ä–∞–Ω–æ', '–ø–æ–∑–¥–Ω–æ', '–≤—Ä–µ–º–µ–Ω–Ω–æ–π']
            if any(indicator in meanings for indicator in time_indicators):
                # Keep in time_age if it's genuinely about time
                continue
        
        # 2. Work-related phrases about effort vs results
        effort_result_patterns = [
            r'—É—Å–∏–ª–∏.*—Ä–µ–∑—É–ª—å—Ç–∞—Ç|—Ä–µ–∑—É–ª—å—Ç–∞—Ç.*—É—Å–∏–ª–∏',
            r'—Ç—Ä—É–¥.*–ø–æ–ª—å–∑|–ø–æ–ª—å–∑.*—Ç—Ä—É–¥',
            r'—Ä–∞–±–æ—Ç.*–≤—ã–≥–æ–¥|–≤—ã–≥–æ–¥.*—Ä–∞–±–æ—Ç',
            r'—Å—Ç–∞—Ä–∞—Ç—å—Å—è.*—Ç–æ–ª–∫|—Ç–æ–ª–∫.*—Å—Ç–∞—Ä–∞—Ç—å—Å—è',
            r'–¥–µ–ª–æ.*—Å—Ç–æ–∏—Ç|—Å—Ç–æ–∏—Ç.*–¥–µ–ª–æ'
        ]
        for pattern in effort_result_patterns:
            if re.search(pattern, meanings):
                if best_category != 'work_labor':
                    best_category = 'work_labor'
                    best_score = 8
                    break
        
        # 3. Money/wealth related expressions
        wealth_patterns = [
            r'–±–æ–≥–∞—Ç.*–¥–µ—Ç–µ–π|–¥–µ—Ç–µ–π.*–±–æ–≥–∞—Ç',
            r'–º–∞—Ç–µ—Ä–∏–∞–ª—å–Ω.*–ø–æ–ª–æ–∂–µ–Ω|–ø–æ–ª–æ–∂–µ–Ω.*–º–∞—Ç–µ—Ä–∏–∞–ª—å–Ω',
            r'–¥–µ–Ω—å–≥–∏.*—Ç—Ä–∞—Ç–∏—Ç—å|—Ç—Ä–∞—Ç–∏—Ç—å.*–¥–µ–Ω—å–≥–∏'
        ]
        for pattern in wealth_patterns:
            if re.search(pattern, meanings):
                if best_category != 'money_wealth':
                    best_category = 'money_wealth'
                    best_score = 8
                    break
        
        # If we found a significantly better category
        if best_category and best_category != current_category and best_score >= 5:
            corrections.append({
                'index': i,
                'phrase': phrase_data['phrase'],
                'current_category': current_category,
                'suggested_category': best_category,
                'score': best_score,
                'meaning': phrase_data.get('meanings', [''])[0] if phrase_data.get('meanings') else '',
                'reason': f'Semantic analysis (score: {best_score})',
                'category_scores': category_scores
            })
    
    return corrections

def apply_semantic_corrections(data, corrections, apply_threshold=5):
    """Apply semantic corrections to the data."""
    phrases = data['phrases']
    
    applied_count = 0
    skipped_count = 0
    
    print(f"\nüîÑ Applying corrections with confidence score >= {apply_threshold}...")
    
    for correction in corrections:
        if correction['score'] >= apply_threshold:
            phrase_data = phrases[correction['index']]
            old_category = phrase_data['category']
            phrase_data['category'] = correction['suggested_category']
            
            print(f"‚úÖ '{correction['phrase'][:50]}{'...' if len(correction['phrase']) > 50 else ''}' ")
            print(f"   {old_category} ‚Üí {correction['suggested_category']} (score: {correction['score']})")
            print(f"   Meaning: {correction['meaning'][:80]}{'...' if len(correction['meaning']) > 80 else ''}")
            print()
            
            applied_count += 1
        else:
            skipped_count += 1
    
    print(f"üìä Applied {applied_count} corrections, skipped {skipped_count} low-confidence suggestions")
    return data

def main():
    """Main function to analyze and fix comprehensive semantic categorization."""
    print("üîß Starting comprehensive semantic categorization analysis...")
    
    # Load data
    data = load_phrases()
    
    # Analyze semantic issues
    corrections = analyze_comprehensive_semantic_categorization(data)
    
    print(f"\nüö® Found {len(corrections)} potential semantic corrections")
    
    # Show statistics by category
    current_categories = Counter([c['current_category'] for c in corrections])
    suggested_categories = Counter([c['suggested_category'] for c in corrections])
    
    print(f"\nüìà Categories with most issues:")
    for cat, count in current_categories.most_common(10):
        print(f"  {cat}: {count} phrases need correction")
    
    print(f"\nüìà Most suggested target categories:")
    for cat, count in suggested_categories.most_common(10):
        print(f"  {cat}: {count} phrases should move here")
    
    # Show top corrections by confidence
    print(f"\nüìã Top 20 high-confidence corrections:")
    for correction in sorted(corrections, key=lambda x: x['score'], reverse=True)[:20]:
        print(f"  '{correction['phrase'][:40]}{'...' if len(correction['phrase']) > 40 else ''}' ")
        print(f"    {correction['current_category']} ‚Üí {correction['suggested_category']} (score: {correction['score']})")
        print(f"    {correction['meaning'][:60]}{'...' if len(correction['meaning']) > 60 else ''}")
        print()
    
    # Apply high-confidence corrections
    print(f"\nüîÑ Applying high-confidence corrections (score >= 5)...")
    fixed_data = apply_semantic_corrections(data, corrections, apply_threshold=5)
    
    # Save corrected data
    with open('table_phrases.json', 'w', encoding='utf-8') as f:
        json.dump(fixed_data, f, ensure_ascii=False, indent=2)
    
    print(f"\nüíæ Applied corrections to table_phrases.json")
    
    # Also save a backup with all suggestions for review
    with open('semantic_corrections_report.json', 'w', encoding='utf-8') as f:
        json.dump({
            'applied_corrections': [c for c in corrections if c['score'] >= 5],
            'suggested_corrections': [c for c in corrections if c['score'] < 5],
            'statistics': {
                'total_corrections': len(corrections),
                'applied': len([c for c in corrections if c['score'] >= 5]),
                'suggested': len([c for c in corrections if c['score'] < 5]),
                'categories_with_issues': dict(current_categories),
                'target_categories': dict(suggested_categories)
            }
        }, f, ensure_ascii=False, indent=2)
    
    print(f"üìã Detailed report saved to semantic_corrections_report.json")
    
    return corrections

if __name__ == "__main__":
    corrections = main()